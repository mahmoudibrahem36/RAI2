{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116a80ca-60db-474b-a6ec-001559f9a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, SpatialDropout1D, GlobalAveragePooling1D, Layer, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from captum.attr import IntegratedGradients\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cda8f33-74f4-4abb-9ef1-0f872d96f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('Training_Essay_Data.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73402f35-785d-4a2c-8521-b4c215d39f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Cleaning\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0625a2d-e2ca-414a-add9-7937f9c0a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7253437e-9a19-43c3-8500-c3f7f8c35da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, stopwords removal, and lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bbe6259-ea35-474a-9791-ef95d562ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a5c376-1c27-4d65-8462-29721a1ca805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['clean_text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c713050-f073-400c-b684-7ca776dafd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50502678, 54888100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train FastText model\n",
    "tokenized_sentences = df['tokens'].tolist()\n",
    "fasttext_model = FastText(vector_size=300, window=5, min_count=1, workers=4)\n",
    "fasttext_model.build_vocab(corpus_iterable=tokenized_sentences)\n",
    "fasttext_model.train(corpus_iterable=tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcda8ab0-e625-4496-aa31-d8cc1ebe578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load GloVe embeddings\n",
    "def load_glove_embeddings(filepath):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a09e1194-1f79-4a7f-9a74-3ef61b591150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download and load GloVe embeddings\n",
    "def download_glove_embeddings(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96821914-fe4a-47d4-9994-3743998f3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unzip_glove_embeddings(zip_filepath, extract_dir):\n",
    "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c286cbe1-c3aa-4a46-82fd-9369a50da411",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "glove_zip_filepath = 'glove.6B.zip'\n",
    "glove_extract_dir = 'glove.6B'\n",
    "glove_embedding_filepath = 'glove.6B/glove.6B.100d.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "535bc0bc-ba4b-4455-beff-b5fbefbe4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_glove_embeddings(glove_url, glove_zip_filepath)\n",
    "unzip_glove_embeddings(glove_zip_filepath, glove_extract_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c45044-158f-4e8b-8f1f-a5f01654aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = load_glove_embeddings(glove_embedding_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db5f45d-a502-4bfe-8665-212ec121f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical representations: Padding\n",
    "max_seq_length = 100\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "X_seq = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_seq_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aab9673-74d9-46b4-be1f-a503abf4b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "X = X_pad\n",
    "y = df['generated']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d93d3950-0e4d-429a-b8f0-ebab1e4c5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e817ca57-f4f7-489b-a388-759717b79e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Attention layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, return_sequences=True):\n",
    "        super(Attention, self).__init__()\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.activations.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        a = tf.keras.activations.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c06d0ea2-44f1-4b60-a067-d22c976171f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "def create_embedding_matrix(tokenizer, model, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in model.wv:\n",
    "            embedding_matrix[i] = model.wv[word]\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9f17e22-c490-4506-88cd-b8f0c26867db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrices for FastText and GloVe\n",
    "embedding_matrix_fasttext = create_embedding_matrix(tokenizer, fasttext_model, 300)\n",
    "embedding_matrix_glove = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_glove[i] = embedding_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69f7dc83-c675-41c3-912c-2f6293d21aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model with embeddings and attention\n",
    "def create_lstm_model(input_dim, output_dim, max_sequence_length, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, output_dim, input_length=max_sequence_length,\n",
    "                        embeddings_initializer=Constant(embedding_matrix),\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Attention(return_sequences=True))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c748ae7-601c-49af-948f-a1d774982451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmedma\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM models with different embeddings\n",
    "lstm_model_fasttext = create_lstm_model(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "lstm_model_glove = create_lstm_model(len(tokenizer.word_index) + 1, 100, max_seq_length, embedding_matrix_glove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e40a31-9f55-4d8c-8ae4-bf83361dd11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:From C:\\Users\\Ahmedma\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 45ms/step - accuracy: 0.8081 - loss: 0.5910 - val_accuracy: 0.9458 - val_loss: 0.3915\n",
      "Epoch 2/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9337 - loss: 0.3686 - val_accuracy: 0.9555 - val_loss: 0.2730\n",
      "Epoch 3/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.9450 - loss: 0.2695 - val_accuracy: 0.9557 - val_loss: 0.2163\n",
      "Epoch 4/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - accuracy: 0.9588 - loss: 0.2045 - val_accuracy: 0.9632 - val_loss: 0.1776\n",
      "Epoch 5/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 46ms/step - accuracy: 0.9590 - loss: 0.1726 - val_accuracy: 0.9672 - val_loss: 0.1402\n",
      "Epoch 6/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.9628 - loss: 0.1490 - val_accuracy: 0.9718 - val_loss: 0.1244\n",
      "Epoch 7/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.9644 - loss: 0.1326 - val_accuracy: 0.9720 - val_loss: 0.1080\n",
      "Epoch 8/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9699 - loss: 0.1135 - val_accuracy: 0.9748 - val_loss: 0.0968\n",
      "Epoch 9/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 47ms/step - accuracy: 0.9731 - loss: 0.0992 - val_accuracy: 0.9754 - val_loss: 0.0911\n",
      "Epoch 10/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9743 - loss: 0.0903 - val_accuracy: 0.9768 - val_loss: 0.0840\n",
      "Epoch 11/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.9762 - loss: 0.0861 - val_accuracy: 0.9758 - val_loss: 0.0813\n",
      "Epoch 12/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9777 - loss: 0.0802 - val_accuracy: 0.9748 - val_loss: 0.0808\n",
      "Epoch 13/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9802 - loss: 0.0684 - val_accuracy: 0.9759 - val_loss: 0.0741\n",
      "Epoch 14/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9815 - loss: 0.0634 - val_accuracy: 0.9769 - val_loss: 0.0744\n",
      "Epoch 15/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9796 - loss: 0.0656 - val_accuracy: 0.9787 - val_loss: 0.0692\n",
      "Epoch 16/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9830 - loss: 0.0570 - val_accuracy: 0.9775 - val_loss: 0.0720\n",
      "Epoch 17/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - accuracy: 0.9814 - loss: 0.0591 - val_accuracy: 0.9774 - val_loss: 0.0696\n",
      "Epoch 18/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9844 - loss: 0.0517 - val_accuracy: 0.9787 - val_loss: 0.0670\n",
      "Epoch 19/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9867 - loss: 0.0447 - val_accuracy: 0.9776 - val_loss: 0.0675\n",
      "Epoch 20/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - accuracy: 0.9864 - loss: 0.0460 - val_accuracy: 0.9793 - val_loss: 0.0686\n",
      "Epoch 21/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9872 - loss: 0.0432 - val_accuracy: 0.9800 - val_loss: 0.0660\n",
      "Epoch 22/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.9883 - loss: 0.0391 - val_accuracy: 0.9800 - val_loss: 0.0623\n",
      "Epoch 23/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - accuracy: 0.9887 - loss: 0.0362 - val_accuracy: 0.9815 - val_loss: 0.0614\n",
      "Epoch 24/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - accuracy: 0.9893 - loss: 0.0358 - val_accuracy: 0.9818 - val_loss: 0.0595\n",
      "Epoch 25/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - accuracy: 0.9892 - loss: 0.0330 - val_accuracy: 0.9808 - val_loss: 0.0628\n",
      "Epoch 26/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 45ms/step - accuracy: 0.9919 - loss: 0.0301 - val_accuracy: 0.9814 - val_loss: 0.0598\n",
      "Epoch 27/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9905 - loss: 0.0305 - val_accuracy: 0.9809 - val_loss: 0.0629\n",
      "Epoch 28/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - accuracy: 0.9906 - loss: 0.0301 - val_accuracy: 0.9810 - val_loss: 0.0602\n",
      "Epoch 29/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9924 - loss: 0.0263 - val_accuracy: 0.9818 - val_loss: 0.0586\n",
      "Epoch 30/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9926 - loss: 0.0261 - val_accuracy: 0.9806 - val_loss: 0.0606\n",
      "Epoch 31/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9916 - loss: 0.0301 - val_accuracy: 0.9834 - val_loss: 0.0553\n",
      "Epoch 32/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9921 - loss: 0.0248 - val_accuracy: 0.9791 - val_loss: 0.0649\n",
      "Epoch 33/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9913 - loss: 0.0276 - val_accuracy: 0.9816 - val_loss: 0.0594\n",
      "Epoch 34/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.9928 - loss: 0.0225 - val_accuracy: 0.9806 - val_loss: 0.0656\n",
      "Epoch 35/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9929 - loss: 0.0230 - val_accuracy: 0.9809 - val_loss: 0.0609\n",
      "Epoch 36/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.9941 - loss: 0.0201 - val_accuracy: 0.9810 - val_loss: 0.0664\n",
      "Epoch 37/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9933 - loss: 0.0229 - val_accuracy: 0.9803 - val_loss: 0.0638\n",
      "Epoch 38/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9937 - loss: 0.0217 - val_accuracy: 0.9827 - val_loss: 0.0558\n",
      "Epoch 39/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 43ms/step - accuracy: 0.9951 - loss: 0.0168 - val_accuracy: 0.9821 - val_loss: 0.0559\n",
      "Epoch 40/40\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 44ms/step - accuracy: 0.9949 - loss: 0.0159 - val_accuracy: 0.9826 - val_loss: 0.0573\n"
     ]
    }
   ],
   "source": [
    "# Train the LSTM model with FastText embeddings\n",
    "history_fasttext = lstm_model_fasttext.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=40, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3da30a1b-d4dc-4676-90aa-0912372f5b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      5234\n",
      "           1       0.98      0.97      0.98      3028\n",
      "\n",
      "    accuracy                           0.98      8262\n",
      "   macro avg       0.98      0.98      0.98      8262\n",
      "weighted avg       0.98      0.98      0.98      8262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the LSTM model on the test set\n",
    "y_pred_fasttext = (lstm_model_fasttext.predict(X_val) > 0.5).astype(int)\n",
    "print(classification_report(y_val, y_pred_fasttext))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dd95cb1-d9cf-4d88-8549-058e8c51aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "lstm_model_fasttext.save('lstm_model_fasttext.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e54018e-e7cc-49ce-b6c2-281674c1e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional steps for privacy and security:\n",
    "\n",
    "# 1. Data Anonymization\n",
    "# Remove sensitive columns like 'user_id', 'email', etc., if present.\n",
    "#df_anonymized = df.drop(columns=['user_id', 'email'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "033af55e-8858-43ab-9a23-abdc1dfe22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7270634f-47bd-45f9-a366-ab573896bb6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'compute_dp_sgd_privacy' from 'tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy' (C:\\Users\\Ahmedma\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow_privacy\\privacy\\analysis\\compute_dp_sgd_privacy.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[189], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute_dp_sgd_privacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_dp_sgd_privacy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdp_optimizer_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_optimizer_class\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'compute_dp_sgd_privacy' from 'tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy' (C:\\Users\\Ahmedma\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow_privacy\\privacy\\analysis\\compute_dp_sgd_privacy.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import make_optimizer_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "78c45a35-31b6-46f8-9191-6c98a65a7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define differential privacy parameters\n",
    "learning_rate = 0.001\n",
    "noise_multiplier = 1.1\n",
    "l2_norm_clip = 1.0\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "37046874-4280-4468-845e-1c29d6e1ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3a44e10d-4de4-4242-b649-420d993f39d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved epsilon: 1.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_dp_epsilon(samples, batch_size, noise_multiplier, epochs, delta):\n",
    "    \"\"\"Compute epsilon for differential privacy.\"\"\"\n",
    "    if noise_multiplier == 0.0:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Gaussian mechanism formula for epsilon calculation\n",
    "    sigma = noise_multiplier * l2_norm_clip / batch_size\n",
    "    epsilon = (sigma * math.sqrt(2 * epochs * math.log(1/delta))) + (math.exp(sigma) - 1) * (epochs / samples)\n",
    "    \n",
    "    return epsilon\n",
    "\n",
    "# Example parameters\n",
    "samples = len(X_train)\n",
    "batch_size = 32\n",
    "noise_multiplier = 1.1\n",
    "epochs = 40\n",
    "delta = 1e-5\n",
    "\n",
    "# Compute epsilon\n",
    "epsilon = compute_dp_epsilon(samples, batch_size, noise_multiplier, epochs, delta)\n",
    "print(f\"Achieved epsilon: {epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "90ef752c-88f8-4ed4-81e8-ee860e9db7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your model (example LSTM model with FastText embeddings)\n",
    "def create_lstm_model(input_dim, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim, embedding_dim, weights=[embedding_matrix],\n",
    "                                  input_length=max_seq_length, trainable=False),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0d45b1af-7f0f-44a1-a38c-4a4eead0ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Assuming you have tokenizer and embedding_matrix_fasttext defined\n",
    "lstm_model_fasttext_dp = create_lstm_model(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d3c26b09-4c42-4414-b7e5-4ad8d52ca0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a native TensorFlow optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "lstm_model_fasttext_dp.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fd3ba397-7fa6-4ceb-916f-0d6ae1c9e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop with differential privacy\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e9a0415e-ed72-49c1-8629-52ba996740be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2b5f4d1f-43a2-4a6a-809c-638c3744dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clip gradients and add noise\n",
    "def apply_dp_noise_and_clip(grads_and_vars, l2_norm_clip, noise_multiplier):\n",
    "    clipped_gradients = []\n",
    "    for gradient, variable in grads_and_vars:\n",
    "        if gradient is not None:\n",
    "            noise = tf.random.normal(tf.shape(gradient), stddev=noise_multiplier * l2_norm_clip)\n",
    "            noised_gradient = gradient + noise\n",
    "            clipped_gradient = tf.clip_by_norm(noised_gradient, l2_norm_clip)\n",
    "            clipped_gradients.append((clipped_gradient, variable))\n",
    "        else:\n",
    "            clipped_gradients.append((gradient, variable))\n",
    "    return clipped_gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2b762d2a-66d0-4a98-98a7-eeda754e0e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "Step 0, Loss: 0.7195061445236206\n",
      "Step 100, Loss: 0.7181477546691895\n",
      "Step 200, Loss: 0.6606565713882446\n",
      "Step 300, Loss: 0.5793500542640686\n",
      "Step 400, Loss: 0.5711864829063416\n",
      "Step 500, Loss: 0.6737239360809326\n",
      "Step 600, Loss: 0.6145614385604858\n",
      "Validation Loss: 0.6185742616653442\n",
      "Epoch 2/40\n",
      "Step 0, Loss: 0.6071523427963257\n",
      "Step 100, Loss: 0.647794246673584\n",
      "Step 200, Loss: 0.6053821444511414\n",
      "Step 300, Loss: 0.5587713122367859\n",
      "Step 400, Loss: 0.6395576000213623\n",
      "Step 500, Loss: 0.6156624555587769\n",
      "Step 600, Loss: 0.5641543865203857\n",
      "Validation Loss: 0.5822763442993164\n",
      "Epoch 3/40\n",
      "Step 0, Loss: 0.5897873640060425\n",
      "Step 100, Loss: 0.5177196264266968\n",
      "Step 200, Loss: 0.5647162199020386\n",
      "Step 300, Loss: 0.5352736115455627\n",
      "Step 400, Loss: 0.6954275369644165\n",
      "Step 500, Loss: 0.636247456073761\n",
      "Step 600, Loss: 0.5569354295730591\n",
      "Validation Loss: 0.5632286071777344\n",
      "Epoch 4/40\n",
      "Step 0, Loss: 0.6082323789596558\n",
      "Step 100, Loss: 0.46491801738739014\n",
      "Step 200, Loss: 0.45711013674736023\n",
      "Step 300, Loss: 0.5593241453170776\n",
      "Step 400, Loss: 0.5445253252983093\n",
      "Step 500, Loss: 0.5811106562614441\n",
      "Step 600, Loss: 0.49484652280807495\n",
      "Validation Loss: 0.5544837117195129\n",
      "Epoch 5/40\n",
      "Step 0, Loss: 0.541551947593689\n",
      "Step 100, Loss: 0.4860284924507141\n",
      "Step 200, Loss: 0.5545201897621155\n",
      "Step 300, Loss: 0.43348419666290283\n",
      "Step 400, Loss: 0.5809348225593567\n",
      "Step 500, Loss: 0.5905787944793701\n",
      "Step 600, Loss: 0.6052536964416504\n",
      "Validation Loss: 0.5518152117729187\n",
      "Epoch 6/40\n",
      "Step 0, Loss: 0.5092182755470276\n",
      "Step 100, Loss: 0.5790297985076904\n",
      "Step 200, Loss: 0.5543161630630493\n",
      "Step 300, Loss: 0.5081533193588257\n",
      "Step 400, Loss: 0.5077947378158569\n",
      "Step 500, Loss: 0.6278729438781738\n",
      "Step 600, Loss: 0.5286502242088318\n",
      "Validation Loss: 0.5458467602729797\n",
      "Epoch 7/40\n",
      "Step 0, Loss: 0.4647340178489685\n",
      "Step 100, Loss: 0.5110360383987427\n",
      "Step 200, Loss: 0.558286190032959\n",
      "Step 300, Loss: 0.6291989088058472\n",
      "Step 400, Loss: 0.6141618490219116\n",
      "Step 500, Loss: 0.5243744254112244\n",
      "Step 600, Loss: 0.5240656137466431\n",
      "Validation Loss: 0.5400159358978271\n",
      "Epoch 8/40\n",
      "Step 0, Loss: 0.579231858253479\n",
      "Step 100, Loss: 0.5557220578193665\n",
      "Step 200, Loss: 0.4552210867404938\n",
      "Step 300, Loss: 0.44328778982162476\n",
      "Step 400, Loss: 0.6410184502601624\n",
      "Step 500, Loss: 0.5112147331237793\n",
      "Step 600, Loss: 0.5203915238380432\n",
      "Validation Loss: 0.5276626348495483\n",
      "Epoch 9/40\n",
      "Step 0, Loss: 0.6882413029670715\n",
      "Step 100, Loss: 0.41156014800071716\n",
      "Step 200, Loss: 0.5131527185440063\n",
      "Step 300, Loss: 0.4677916467189789\n",
      "Step 400, Loss: 0.6195992231369019\n",
      "Step 500, Loss: 0.5150251388549805\n",
      "Step 600, Loss: 0.5368028283119202\n",
      "Validation Loss: 0.5252174139022827\n",
      "Epoch 10/40\n",
      "Step 0, Loss: 0.5500413775444031\n",
      "Step 100, Loss: 0.46669596433639526\n",
      "Step 200, Loss: 0.537801206111908\n",
      "Step 300, Loss: 0.5252319574356079\n",
      "Step 400, Loss: 0.5824784636497498\n",
      "Step 500, Loss: 0.46351000666618347\n",
      "Step 600, Loss: 0.45661085844039917\n",
      "Validation Loss: 0.5193045139312744\n",
      "Epoch 11/40\n",
      "Step 0, Loss: 0.5554814338684082\n",
      "Step 100, Loss: 0.4188010096549988\n",
      "Step 200, Loss: 0.4948951005935669\n",
      "Step 300, Loss: 0.6394455432891846\n",
      "Step 400, Loss: 0.5749483108520508\n",
      "Step 500, Loss: 0.5247576236724854\n",
      "Step 600, Loss: 0.42723965644836426\n",
      "Validation Loss: 0.5128986239433289\n",
      "Epoch 12/40\n",
      "Step 0, Loss: 0.5193490386009216\n",
      "Step 100, Loss: 0.4776081442832947\n",
      "Step 200, Loss: 0.5855141878128052\n",
      "Step 300, Loss: 0.5223042964935303\n",
      "Step 400, Loss: 0.5287736654281616\n",
      "Step 500, Loss: 0.6252028942108154\n",
      "Step 600, Loss: 0.4929872453212738\n",
      "Validation Loss: 0.5073283314704895\n",
      "Epoch 13/40\n",
      "Step 0, Loss: 0.4292539954185486\n",
      "Step 100, Loss: 0.4511324167251587\n",
      "Step 200, Loss: 0.47226327657699585\n",
      "Step 300, Loss: 0.6868931651115417\n",
      "Step 400, Loss: 0.48583438992500305\n",
      "Step 500, Loss: 0.4003369212150574\n",
      "Step 600, Loss: 0.5365967154502869\n",
      "Validation Loss: 0.5033304691314697\n",
      "Epoch 14/40\n",
      "Step 0, Loss: 0.545501708984375\n",
      "Step 100, Loss: 0.542705237865448\n",
      "Step 200, Loss: 0.5902432799339294\n",
      "Step 300, Loss: 0.437544047832489\n",
      "Step 400, Loss: 0.5793034434318542\n",
      "Step 500, Loss: 0.5314537286758423\n",
      "Step 600, Loss: 0.6003212928771973\n",
      "Validation Loss: 0.4977926015853882\n",
      "Epoch 15/40\n",
      "Step 0, Loss: 0.6058805584907532\n",
      "Step 100, Loss: 0.4760238230228424\n",
      "Step 200, Loss: 0.41949331760406494\n",
      "Step 300, Loss: 0.47776928544044495\n",
      "Step 400, Loss: 0.5981353521347046\n",
      "Step 500, Loss: 0.456759512424469\n",
      "Step 600, Loss: 0.37673869729042053\n",
      "Validation Loss: 0.4942563772201538\n",
      "Epoch 16/40\n",
      "Step 0, Loss: 0.408463716506958\n",
      "Step 100, Loss: 0.4872989058494568\n",
      "Step 200, Loss: 0.6169817447662354\n",
      "Step 300, Loss: 0.5328474044799805\n",
      "Step 400, Loss: 0.44115638732910156\n",
      "Step 500, Loss: 0.40168002247810364\n",
      "Step 600, Loss: 0.5867047309875488\n",
      "Validation Loss: 0.48853710293769836\n",
      "Epoch 17/40\n",
      "Step 0, Loss: 0.40744560956954956\n",
      "Step 100, Loss: 0.4601059556007385\n",
      "Step 200, Loss: 0.4010307192802429\n",
      "Step 300, Loss: 0.4833148121833801\n",
      "Step 400, Loss: 0.41587284207344055\n",
      "Step 500, Loss: 0.5918226838111877\n",
      "Step 600, Loss: 0.3513522446155548\n",
      "Validation Loss: 0.48118671774864197\n",
      "Epoch 18/40\n",
      "Step 0, Loss: 0.4244000017642975\n",
      "Step 100, Loss: 0.5562505722045898\n",
      "Step 200, Loss: 0.4381616711616516\n",
      "Step 300, Loss: 0.5330798625946045\n",
      "Step 400, Loss: 0.47015243768692017\n",
      "Step 500, Loss: 0.42816078662872314\n",
      "Step 600, Loss: 0.5631780028343201\n",
      "Validation Loss: 0.47618329524993896\n",
      "Epoch 19/40\n",
      "Step 0, Loss: 0.46409422159194946\n",
      "Step 100, Loss: 0.4642554521560669\n",
      "Step 200, Loss: 0.47693392634391785\n",
      "Step 300, Loss: 0.4521341025829315\n",
      "Step 400, Loss: 0.5254867076873779\n",
      "Step 500, Loss: 0.4281347990036011\n",
      "Step 600, Loss: 0.5170762538909912\n",
      "Validation Loss: 0.47255852818489075\n",
      "Epoch 20/40\n",
      "Step 0, Loss: 0.5387082099914551\n",
      "Step 100, Loss: 0.5652905106544495\n",
      "Step 200, Loss: 0.42337363958358765\n",
      "Step 300, Loss: 0.400622695684433\n",
      "Step 400, Loss: 0.373184472322464\n",
      "Step 500, Loss: 0.3528427481651306\n",
      "Step 600, Loss: 0.39906251430511475\n",
      "Validation Loss: 0.4684953987598419\n",
      "Epoch 21/40\n",
      "Step 0, Loss: 0.338662326335907\n",
      "Step 100, Loss: 0.3297058939933777\n",
      "Step 200, Loss: 0.5260661840438843\n",
      "Step 300, Loss: 0.6368973851203918\n",
      "Step 400, Loss: 0.5573662519454956\n",
      "Step 500, Loss: 0.37137413024902344\n",
      "Step 600, Loss: 0.37494271993637085\n",
      "Validation Loss: 0.4613771438598633\n",
      "Epoch 22/40\n",
      "Step 0, Loss: 0.33858755230903625\n",
      "Step 100, Loss: 0.41116267442703247\n",
      "Step 200, Loss: 0.41801023483276367\n",
      "Step 300, Loss: 0.3202363848686218\n",
      "Step 400, Loss: 0.5499041676521301\n",
      "Step 500, Loss: 0.44679567217826843\n",
      "Step 600, Loss: 0.4694165587425232\n",
      "Validation Loss: 0.4529353976249695\n",
      "Epoch 23/40\n",
      "Step 0, Loss: 0.3690233826637268\n",
      "Step 100, Loss: 0.5059893131256104\n",
      "Step 200, Loss: 0.46150922775268555\n",
      "Step 300, Loss: 0.39020657539367676\n",
      "Step 400, Loss: 0.4956030249595642\n",
      "Step 500, Loss: 0.44089728593826294\n",
      "Step 600, Loss: 0.4089394211769104\n",
      "Validation Loss: 0.4487181007862091\n",
      "Epoch 24/40\n",
      "Step 0, Loss: 0.3104321360588074\n",
      "Step 100, Loss: 0.3829067349433899\n",
      "Step 200, Loss: 0.5859348773956299\n",
      "Step 300, Loss: 0.5238285064697266\n",
      "Step 400, Loss: 0.4615325927734375\n",
      "Step 500, Loss: 0.42899662256240845\n",
      "Step 600, Loss: 0.557347297668457\n",
      "Validation Loss: 0.4390209913253784\n",
      "Epoch 25/40\n",
      "Step 0, Loss: 0.49624040722846985\n",
      "Step 100, Loss: 0.5528455972671509\n",
      "Step 200, Loss: 0.3151155412197113\n",
      "Step 300, Loss: 0.4376574158668518\n",
      "Step 400, Loss: 0.41736823320388794\n",
      "Step 500, Loss: 0.449197918176651\n",
      "Step 600, Loss: 0.5726710557937622\n",
      "Validation Loss: 0.43230071663856506\n",
      "Epoch 26/40\n",
      "Step 0, Loss: 0.5582026839256287\n",
      "Step 100, Loss: 0.6775979995727539\n",
      "Step 200, Loss: 0.3684810698032379\n",
      "Step 300, Loss: 0.297807902097702\n",
      "Step 400, Loss: 0.5738871097564697\n",
      "Step 500, Loss: 0.5106850862503052\n",
      "Step 600, Loss: 0.35609614849090576\n",
      "Validation Loss: 0.4312077760696411\n",
      "Epoch 27/40\n",
      "Step 0, Loss: 0.4975031018257141\n",
      "Step 100, Loss: 0.4433736205101013\n",
      "Step 200, Loss: 0.3495105504989624\n",
      "Step 300, Loss: 0.4130169749259949\n",
      "Step 400, Loss: 0.4644787311553955\n",
      "Step 500, Loss: 0.509533166885376\n",
      "Step 600, Loss: 0.48023051023483276\n",
      "Validation Loss: 0.42435744404792786\n",
      "Epoch 28/40\n",
      "Step 0, Loss: 0.40847405791282654\n",
      "Step 100, Loss: 0.3600234389305115\n",
      "Step 200, Loss: 0.3704356551170349\n",
      "Step 300, Loss: 0.44193726778030396\n",
      "Step 400, Loss: 0.24041569232940674\n",
      "Step 500, Loss: 0.3357623815536499\n",
      "Step 600, Loss: 0.4006500840187073\n",
      "Validation Loss: 0.4193056523799896\n",
      "Epoch 29/40\n",
      "Step 0, Loss: 0.29130250215530396\n",
      "Step 100, Loss: 0.48185113072395325\n",
      "Step 200, Loss: 0.33285728096961975\n",
      "Step 300, Loss: 0.42561036348342896\n",
      "Step 400, Loss: 0.4376915395259857\n",
      "Step 500, Loss: 0.2872980535030365\n",
      "Step 600, Loss: 0.4797894358634949\n",
      "Validation Loss: 0.40329813957214355\n",
      "Epoch 30/40\n",
      "Step 0, Loss: 0.45753419399261475\n",
      "Step 100, Loss: 0.4071413278579712\n",
      "Step 200, Loss: 0.470324844121933\n",
      "Step 300, Loss: 0.3530609607696533\n",
      "Step 400, Loss: 0.35492390394210815\n",
      "Step 500, Loss: 0.2445753514766693\n",
      "Step 600, Loss: 0.3237485885620117\n",
      "Validation Loss: 0.40244755148887634\n",
      "Epoch 31/40\n",
      "Step 0, Loss: 0.3329733908176422\n",
      "Step 100, Loss: 0.41134458780288696\n",
      "Step 200, Loss: 0.5412666201591492\n",
      "Step 300, Loss: 0.4974064230918884\n",
      "Step 400, Loss: 0.3145407736301422\n",
      "Step 500, Loss: 0.36024391651153564\n",
      "Step 600, Loss: 0.17633911967277527\n",
      "Validation Loss: 0.39980101585388184\n",
      "Epoch 32/40\n",
      "Step 0, Loss: 0.32242056727409363\n",
      "Step 100, Loss: 0.40779179334640503\n",
      "Step 200, Loss: 0.37118303775787354\n",
      "Step 300, Loss: 0.4897862374782562\n",
      "Step 400, Loss: 0.48774033784866333\n",
      "Step 500, Loss: 0.40174317359924316\n",
      "Step 600, Loss: 0.4812372624874115\n",
      "Validation Loss: 0.3863624334335327\n",
      "Epoch 33/40\n",
      "Step 0, Loss: 0.2834475338459015\n",
      "Step 100, Loss: 0.5927408933639526\n",
      "Step 200, Loss: 0.37476134300231934\n",
      "Step 300, Loss: 0.4691140651702881\n",
      "Step 400, Loss: 0.43648797273635864\n",
      "Step 500, Loss: 0.40266576409339905\n",
      "Step 600, Loss: 0.2796584665775299\n",
      "Validation Loss: 0.3804236650466919\n",
      "Epoch 34/40\n",
      "Step 0, Loss: 0.2975068688392639\n",
      "Step 100, Loss: 0.3749978542327881\n",
      "Step 200, Loss: 0.30547064542770386\n",
      "Step 300, Loss: 0.38960176706314087\n",
      "Step 400, Loss: 0.4536251425743103\n",
      "Step 500, Loss: 0.3169490694999695\n",
      "Step 600, Loss: 0.2974933981895447\n",
      "Validation Loss: 0.37300166487693787\n",
      "Epoch 35/40\n",
      "Step 0, Loss: 0.2840910255908966\n",
      "Step 100, Loss: 0.33607596158981323\n",
      "Step 200, Loss: 0.24424466490745544\n",
      "Step 300, Loss: 0.29139548540115356\n",
      "Step 400, Loss: 0.37392938137054443\n",
      "Step 500, Loss: 0.33457595109939575\n",
      "Step 600, Loss: 0.25510647892951965\n",
      "Validation Loss: 0.3707112669944763\n",
      "Epoch 36/40\n",
      "Step 0, Loss: 0.39553067088127136\n",
      "Step 100, Loss: 0.2564828395843506\n",
      "Step 200, Loss: 0.38228917121887207\n",
      "Step 300, Loss: 0.43557310104370117\n",
      "Step 400, Loss: 0.3724411427974701\n",
      "Step 500, Loss: 0.3520897626876831\n",
      "Step 600, Loss: 0.29406675696372986\n",
      "Validation Loss: 0.37552863359451294\n",
      "Epoch 37/40\n",
      "Step 0, Loss: 0.24307304620742798\n",
      "Step 100, Loss: 0.34102779626846313\n",
      "Step 200, Loss: 0.496995210647583\n",
      "Step 300, Loss: 0.44052737951278687\n",
      "Step 400, Loss: 0.2497280091047287\n",
      "Step 500, Loss: 0.4388514459133148\n",
      "Step 600, Loss: 0.37055906653404236\n",
      "Validation Loss: 0.37222063541412354\n",
      "Epoch 38/40\n",
      "Step 0, Loss: 0.2759920060634613\n",
      "Step 100, Loss: 0.25961875915527344\n",
      "Step 200, Loss: 0.3081226944923401\n",
      "Step 300, Loss: 0.2842971086502075\n",
      "Step 400, Loss: 0.35863015055656433\n",
      "Step 500, Loss: 0.3216184377670288\n",
      "Step 600, Loss: 0.4802132844924927\n",
      "Validation Loss: 0.36724090576171875\n",
      "Epoch 39/40\n",
      "Step 0, Loss: 0.3428534269332886\n",
      "Step 100, Loss: 0.3725023865699768\n",
      "Step 200, Loss: 0.34326493740081787\n",
      "Step 300, Loss: 0.3386632204055786\n",
      "Step 400, Loss: 0.34147360920906067\n",
      "Step 500, Loss: 0.346994549036026\n",
      "Step 600, Loss: 0.422324538230896\n",
      "Validation Loss: 0.36124387383461\n",
      "Epoch 40/40\n",
      "Step 0, Loss: 0.31305551528930664\n",
      "Step 100, Loss: 0.5556042790412903\n",
      "Step 200, Loss: 0.2831767797470093\n",
      "Step 300, Loss: 0.33102989196777344\n",
      "Step 400, Loss: 0.4602513909339905\n",
      "Step 500, Loss: 0.45037275552749634\n",
      "Step 600, Loss: 0.6478206515312195\n",
      "Validation Loss: 0.3537291884422302\n"
     ]
    }
   ],
   "source": [
    "# Training loop with differential privacy\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Training step\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = lstm_model_fasttext_dp(x_batch_train, training=True)\n",
    "            loss = loss_fn(y_batch_train, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, lstm_model_fasttext_dp.trainable_variables)\n",
    "        clipped_gradients = apply_dp_noise_and_clip(zip(gradients, lstm_model_fasttext_dp.trainable_variables), l2_norm_clip, noise_multiplier)\n",
    "        \n",
    "        # Apply gradients to model\n",
    "        optimizer.apply_gradients(clipped_gradients)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    # Validation step\n",
    "    val_loss = 0\n",
    "    val_steps = 0\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_predictions = lstm_model_fasttext_dp(x_batch_val, training=False)\n",
    "        val_loss += tf.reduce_mean(loss_fn(y_batch_val, val_predictions))\n",
    "        val_steps += 1\n",
    "\n",
    "    val_loss /= val_steps\n",
    "    print(f\"Validation Loss: {val_loss.numpy()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "68cb38e0-782d-47d8-a89b-09d269639e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the privacy budget\n",
    "def compute_dp_epsilon(samples, batch_size, noise_multiplier, epochs, delta):\n",
    "    if noise_multiplier == 0.0:\n",
    "        return float('inf')\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "    rdp = compute_rdp(q=samples / len(X_train), noise_multiplier=noise_multiplier, steps=epochs, orders=orders)\n",
    "    eps, _, _ = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "    return eps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8ab177d1-9500-4ff9-89ad-7595e9e34ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = len(X_train)\n",
    "delta = 1e-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9ad46449-0ecf-4858-a7fe-c2e4750611a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_privacy.privacy.analysis.rdp_accountant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[209], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdp_optimizer_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DPKerasAdamOptimizer\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdp_accountant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_rdp\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdp_accountant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_privacy_spent\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_privacy.privacy.analysis.rdp_accountant'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasAdamOptimizer\n",
    "from tensorflow_privacy.privacy.analysis import compute_rdp, get_privacy_spent\n",
    "\n",
    "# Define differential privacy parameters\n",
    "learning_rate = 0.001\n",
    "noise_multiplier = 1.1\n",
    "l2_norm_clip = 1.0\n",
    "batch_size = 32\n",
    "num_microbatches = batch_size\n",
    "epochs = 40\n",
    "\n",
    "# Create the DPKerasAdamOptimizer\n",
    "dp_optimizer = DPKerasAdamOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Create your model\n",
    "lstm_model_fasttext_dp = create_lstm_model(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "\n",
    "# Compile the model with the differentially private optimizer\n",
    "lstm_model_fasttext_dp.compile(optimizer=dp_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Custom training loop\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # Training step\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = lstm_model_fasttext_dp(x_batch_train, training=True)\n",
    "            loss = loss_fn(y_batch_train, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, lstm_model_fasttext_dp.trainable_variables)\n",
    "        clipped_gradients = [tf.clip_by_norm(g, l2_norm_clip) for g in gradients]\n",
    "        \n",
    "        # Explicitly call apply_gradients on the optimizer\n",
    "        dp_optimizer.apply_gradients(zip(clipped_gradients, lstm_model_fasttext_dp.trainable_variables))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy()}\")\n",
    "    \n",
    "    # Validation step\n",
    "    val_loss = 0\n",
    "    val_steps = 0\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_predictions = lstm_model_fasttext_dp(x_batch_val, training=False)\n",
    "        val_loss += tf.reduce_mean(loss_fn(y_batch_val, val_predictions))\n",
    "        val_steps += 1\n",
    "\n",
    "    val_loss /= val_steps\n",
    "    print(f\"Validation Loss: {val_loss.numpy()}\")\n",
    "\n",
    "# Compute the privacy budget\n",
    "def compute_dp_epsilon(samples, batch_size, noise_multiplier, epochs, delta):\n",
    "    if noise_multiplier == 0.0:\n",
    "        return float('inf')\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "    rdp = compute_rdp(q=samples / len(X_train), noise_multiplier=noise_multiplier, steps=epochs * (samples // batch_size), orders=orders)\n",
    "    eps, _, _ = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "    return eps\n",
    "\n",
    "samples = len(X_train)\n",
    "delta = 1e-5\n",
    "\n",
    "# Compute epsilon\n",
    "epsilon = compute_dp_epsilon(samples, batch_size, noise_multiplier, epochs, delta)\n",
    "print(f\"Achieved epsilon: {epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e1c3d3ff-badd-46f9-82b4-a619a5a0af17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_privacy.privacy.analysis.rdp_accountant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[208], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute epsilon\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m compute_dp_epsilon(samples, batch_size, noise_multiplier, epochs, delta)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAchieved epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[206], line 6\u001b[0m, in \u001b[0;36mcompute_dp_epsilon\u001b[1;34m(samples, batch_size, noise_multiplier, epochs, delta)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m orders \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10.\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m)] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m rdp \u001b[38;5;241m=\u001b[39m compute_rdp(q\u001b[38;5;241m=\u001b[39msamples \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train), noise_multiplier\u001b[38;5;241m=\u001b[39mnoise_multiplier, steps\u001b[38;5;241m=\u001b[39mepochs, orders\u001b[38;5;241m=\u001b[39morders)\n\u001b[0;32m      7\u001b[0m eps, _, _ \u001b[38;5;241m=\u001b[39m get_privacy_spent(orders, rdp, target_delta\u001b[38;5;241m=\u001b[39mdelta)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eps\n",
      "Cell \u001b[1;32mIn[194], line 12\u001b[0m, in \u001b[0;36mcompute_rdp\u001b[1;34m(q, noise_multiplier, steps, orders)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_rdp\u001b[39m(q, noise_multiplier, steps, orders):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute RDP for given hyperparameters.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_privacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprivacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdp_accountant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_rdp\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_rdp(q, noise_multiplier, steps, orders)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_privacy.privacy.analysis.rdp_accountant'"
     ]
    }
   ],
   "source": [
    "# Compute epsilon\n",
    "epsilon = compute_dp_epsilon(samples, batch_size, noise_multiplier, epochs, delta)\n",
    "print(f\"Achieved epsilon: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4ea658b8-1b12-4fd6-a42b-4fe4e339502b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[203], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute the privacy budget\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m epsilon, _ \u001b[38;5;241m=\u001b[39m compute_dp_sgd_privacy(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_train), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, noise_multiplier\u001b[38;5;241m=\u001b[39mnoise_multiplier, epochs\u001b[38;5;241m=\u001b[39mepochs, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAchieved epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# Compute the privacy budget\n",
    "epsilon, _ = compute_dp_sgd_privacy(n=len(X_train), batch_size=batch_size, noise_multiplier=noise_multiplier, epochs=epochs, delta=1e-5)\n",
    "print(f\"Achieved epsilon: {epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7bd6ec78-e539-4cee-90ff-ec45435d1909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m259/259\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step\n",
      "Differentially Private LSTM Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.90      5234\n",
      "           1       0.83      0.79      0.81      3028\n",
      "\n",
      "    accuracy                           0.87      8262\n",
      "   macro avg       0.86      0.85      0.85      8262\n",
      "weighted avg       0.86      0.87      0.87      8262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the differentially private LSTM model\n",
    "y_pred_fasttext_dp = (lstm_model_fasttext_dp.predict(X_val) > 0.5).astype(int)\n",
    "print(\"Differentially Private LSTM Model:\")\n",
    "print(classification_report(y_val, y_pred_fasttext_dp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2dffd993-323c-4d82-bb57-c4c359494dfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compute privacy budget\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m epsilon, _ \u001b[38;5;241m=\u001b[39m compute_dp_sgd_privacy(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_train), batch_size\u001b[38;5;241m=\u001b[39mbatch_size, noise_multiplier\u001b[38;5;241m=\u001b[39mnoise_multiplier, epochs\u001b[38;5;241m=\u001b[39mepochs, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrivacy budget (ε, δ=1e-5): ε = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute privacy budget\n",
    "epsilon, _ = compute_dp_sgd_privacy(n=len(X_train), batch_size=batch_size, noise_multiplier=noise_multiplier, epochs=epochs, delta=1e-5)\n",
    "print(f\"Privacy budget (ε, δ=1e-5): ε = {epsilon:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Secure Model Deployment\n",
    "# Ensure the model is deployed in a secure environment using HTTPS\n",
    "\n",
    "# 4. Data Encryption\n",
    "# Encrypt sensitive data both at rest and in transit\n",
    "\n",
    "# These steps help integrate privacy and security into your text classification project, ensuring compliance with responsible AI practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "802daf4d-df99-4ab5-bb73-2e5be4bbe3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from Flask) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from Flask) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from Flask) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from Flask) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from click>=8.0->Flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->Flask) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install Flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "de3db088-3ddb-46c5-b6ea-fc1ab2f72851",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when deserializing class 'Attention' using config={'return_sequences': True, 'trainable': True, 'dtype': 'float32'}.\n\nException encountered: Unrecognized keyword arguments passed to Attention: {'return_sequences': True}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:208\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\attention\\attention.py:73\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[1;34m(self, use_scale, score_mode, dropout, seed, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     67\u001b[0m     use_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     72\u001b[0m ):\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_scale \u001b[38;5;241m=\u001b[39m use_scale\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Attention: {'return_sequences': True}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[211], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the LSTM model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_model_fasttext.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Adjust the path to your saved model file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Function for text preprocessing\u001b[39;00m\n\u001b[0;32m     13\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Adjust according to your model's input shape\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    177\u001b[0m         filepath,\n\u001b[0;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    184\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    185\u001b[0m     )\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    134\u001b[0m         model_config, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[0;32m    135\u001b[0m     )\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m serialization\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m     86\u001b[0m     config,\n\u001b[0;32m     87\u001b[0m     module_objects\u001b[38;5;241m=\u001b[39mMODULE_OBJECTS\u001b[38;5;241m.\u001b[39mALL_OBJECTS,\n\u001b[0;32m     88\u001b[0m     custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m     89\u001b[0m     printable_module_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:495\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    490\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(\n\u001b[0;32m    491\u001b[0m     cls_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m--> 495\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(\n\u001b[0;32m    496\u001b[0m         cls_config,\n\u001b[0;32m    497\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    498\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobject_registration\u001b[38;5;241m.\u001b[39mGLOBAL_CUSTOM_OBJECTS,\n\u001b[0;32m    499\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom_objects,\n\u001b[0;32m    500\u001b[0m         },\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\sequential.py:333\u001b[0m, in \u001b[0;36mSequential.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_config \u001b[38;5;129;01min\u001b[39;00m layer_configs:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_config:\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;66;03m# Legacy format deserialization (no \"module\" key)\u001b[39;00m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;66;03m# used for H5 and SavedModel formats\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m         layer \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[0;32m    334\u001b[0m             layer_config,\n\u001b[0;32m    335\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    336\u001b[0m         )\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m         layer \u001b[38;5;241m=\u001b[39m serialization_lib\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m    339\u001b[0m             layer_config,\n\u001b[0;32m    340\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    341\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m serialization\u001b[38;5;241m.\u001b[39mdeserialize_keras_object(\n\u001b[0;32m     86\u001b[0m     config,\n\u001b[0;32m     87\u001b[0m     module_objects\u001b[38;5;241m=\u001b[39mMODULE_OBJECTS\u001b[38;5;241m.\u001b[39mALL_OBJECTS,\n\u001b[0;32m     88\u001b[0m     custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m     89\u001b[0m     printable_module_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:504\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n\u001b[1;32m--> 504\u001b[0m             deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(cls_config)\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# Then `cls` may be a function returning a class.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;66;03m# in this case by convention `config` holds\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# the kwargs of the function.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     custom_objects \u001b[38;5;241m=\u001b[39m custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:210\u001b[0m, in \u001b[0;36mOperation.from_config\u001b[1;34m(cls, config)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when deserializing class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Error when deserializing class 'Attention' using config={'return_sequences': True, 'trainable': True, 'dtype': 'float32'}.\n\nException encountered: Unrecognized keyword arguments passed to Attention: {'return_sequences': True}"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the LSTM model\n",
    "model = load_model('lstm_model_fasttext.h5')  # Adjust the path to your saved model file\n",
    "\n",
    "# Function for text preprocessing\n",
    "max_seq_length = 100  # Adjust according to your model's input shape\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Implement your text preprocessing steps here\n",
    "    return text\n",
    "\n",
    "# Define route for model prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    text = data['text']\n",
    "\n",
    "    # Preprocess the input text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "\n",
    "    # Tokenize and pad the preprocessed text\n",
    "    sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
    "    X_pad = pad_sequences(sequence, maxlen=max_seq_length)\n",
    "\n",
    "    # Make predictions\n",
    "    prediction = model.predict(X_pad)\n",
    "\n",
    "    # Prepare response\n",
    "    result = {\n",
    "        'prediction': prediction.tolist()[0][0]\n",
    "    }\n",
    "\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(ssl_context='adhoc', debug=True)  # Run the app with HTTPS support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9008d-d162-41df-8590-ce7bb7691990",
   "metadata": {},
   "source": [
    "1. Adversarial Training\n",
    "Adversarial training involves generating adversarial examples during the model training process to improve its robustness against adversarial attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3cf29b02-99b8-4c2f-8b1d-960202d78179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cleverhans\n",
      "  Downloading cleverhans-4.0.0-py3-none-any.whl.metadata (846 bytes)\n",
      "Collecting nose (from cleverhans)\n",
      "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pycodestyle in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (2.10.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (1.11.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (3.8.0)\n",
      "Collecting mnist (from cleverhans)\n",
      "  Downloading mnist-0.2.2-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (1.26.4)\n",
      "Requirement already satisfied: tensorflow-probability in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (0.22.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (1.2.0)\n",
      "Collecting easydict (from cleverhans)\n",
      "  Downloading easydict-1.13-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (1.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from cleverhans) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib->cleverhans) (2.8.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-probability->cleverhans) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-probability->cleverhans) (2.2.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-probability->cleverhans) (0.5.4)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\ahmedma\\appdata\\local\\anaconda3\\lib\\site-packages (from tensorflow-probability->cleverhans) (0.1.8)\n",
      "Downloading cleverhans-4.0.0-py3-none-any.whl (92 kB)\n",
      "   ---------------------------------------- 0.0/92.3 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/92.3 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 71.7/92.3 kB 975.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 92.3/92.3 kB 872.6 kB/s eta 0:00:00\n",
      "Downloading easydict-1.13-py3-none-any.whl (6.8 kB)\n",
      "Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 154.7/154.7 kB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: nose, easydict, mnist, cleverhans\n",
      "Successfully installed cleverhans-4.0.0 easydict-1.13 mnist-0.2.2 nose-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1d59bf17-9671-49e7-90ca-4fbfab3b4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "\n",
    "# Define adversarial training function\n",
    "def adversarial_training(model, X_train, y_train):\n",
    "    # Create an FGSM instance\n",
    "    pgd = projected_gradient_descent.ProjectGradientDescent(model, sess=None)\n",
    "\n",
    "    # Train the model with adversarial examples\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(pgd.generate(X_train), y_train, epochs=10, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63617d95-063e-4f61-abbf-ff1f952da222",
   "metadata": {},
   "source": [
    "2. Data Augmentation\n",
    "Data augmentation helps improve model generalization by creating variations of existing data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bd8eba50-8695-4362-9283-bcdc1892a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "# Convert sequences back to text\n",
    "def sequences_to_texts(sequences, tokenizer):\n",
    "    return [' '.join([tokenizer.index_word.get(idx, '') for idx in seq if idx != 0]) for seq in sequences]\n",
    "\n",
    "# Convert texts back to sequences\n",
    "def texts_to_sequences(texts, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Assume tokenizer is already fitted on your data\n",
    "X_train_texts = sequences_to_texts(X_train, tokenizer)\n",
    "\n",
    "# Augment the training data\n",
    "X_augmented_texts = []\n",
    "for sentence in X_train_texts:\n",
    "    augmented_sentence = synonym_replacement(sentence, 2)\n",
    "    X_augmented_texts.append(augmented_sentence)\n",
    "\n",
    "# Convert the augmented data back to sequences\n",
    "X_augmented = texts_to_sequences(X_augmented_texts, tokenizer)\n",
    "\n",
    "# Pad the sequences\n",
    "X_augmented = pad_sequences(X_augmented, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Now you can proceed with the rest of your pipeline using X_augmented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4d9d0-2011-4bc8-8404-cb392b5b30d2",
   "metadata": {},
   "source": [
    "3. Model Regularization\n",
    "Regularization techniques such as dropout and weight regularization help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "01f960cf-4bfc-44cd-9206-49ab31252900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Add dropout layers to the model\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Apply weight regularization\n",
    "model.add(Dense(64, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaca967-57cc-4f43-bafd-a7c814765564",
   "metadata": {},
   "source": [
    "4. Cross-validation\n",
    "Cross-validation ensures the model's performance is consistent across different data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c778b193-4336-443c-964c-4d8c30d9ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    # Train and evaluate the model on each fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0234e1ed-ed22-4eed-90d8-97a680bb7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5368adf9-221b-4886-94ae-13822366c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture with dropout and weight regularization\n",
    "def create_lstm_model_with_regularization(vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                        output_dim=embedding_dim,\n",
    "                                        weights=[embedding_matrix],\n",
    "                                        input_length=max_seq_length,\n",
    "                                        trainable=False))\n",
    "    model.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))  # Add dropout layer\n",
    "    model.add(tf.keras.layers.LSTM(128))\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(0.01), activation='relu'))  # Apply weight regularization\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c0bf2e07-3e63-43d2-b66d-8240512b47d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Epoch 1/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 100ms/step - accuracy: 0.6640 - loss: 0.9375 - val_accuracy: 0.6773 - val_loss: 0.6331\n",
      "Epoch 2/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.6688 - loss: 0.6369 - val_accuracy: 0.6773 - val_loss: 0.6314\n",
      "Epoch 3/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.6661 - loss: 0.6331 - val_accuracy: 0.6773 - val_loss: 0.6320\n",
      "Epoch 4/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.6723 - loss: 0.6203 - val_accuracy: 0.6773 - val_loss: 0.6440\n",
      "Epoch 5/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.6752 - loss: 0.5952 - val_accuracy: 0.6493 - val_loss: 0.6616\n",
      "Epoch 6/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 98ms/step - accuracy: 0.7250 - loss: 0.5544 - val_accuracy: 0.6591 - val_loss: 0.6650\n",
      "Epoch 7/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 98ms/step - accuracy: 0.7733 - loss: 0.4930 - val_accuracy: 0.6036 - val_loss: 0.7451\n",
      "Epoch 8/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 97ms/step - accuracy: 0.8175 - loss: 0.4179 - val_accuracy: 0.5886 - val_loss: 0.8350\n",
      "Epoch 9/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.8528 - loss: 0.3514 - val_accuracy: 0.5829 - val_loss: 0.9163\n",
      "Epoch 10/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.8836 - loss: 0.2930 - val_accuracy: 0.5673 - val_loss: 1.0536\n",
      "Epoch 11/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.9071 - loss: 0.2351 - val_accuracy: 0.6016 - val_loss: 1.1697\n",
      "Epoch 12/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 99ms/step - accuracy: 0.9259 - loss: 0.2033 - val_accuracy: 0.5839 - val_loss: 1.1497\n",
      "Epoch 13/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9288 - loss: 0.1887 - val_accuracy: 0.5777 - val_loss: 1.2936\n",
      "Epoch 14/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.9304 - loss: 0.1804 - val_accuracy: 0.5811 - val_loss: 1.2895\n",
      "Epoch 15/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.9387 - loss: 0.1610 - val_accuracy: 0.5824 - val_loss: 1.4336\n",
      "Epoch 16/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.9469 - loss: 0.1391 - val_accuracy: 0.5709 - val_loss: 1.4681\n",
      "Epoch 17/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 101ms/step - accuracy: 0.9524 - loss: 0.1274 - val_accuracy: 0.5821 - val_loss: 1.4654\n",
      "Epoch 18/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.9574 - loss: 0.1197 - val_accuracy: 0.5844 - val_loss: 1.6229\n",
      "Epoch 19/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.9623 - loss: 0.1081 - val_accuracy: 0.5652 - val_loss: 1.6070\n",
      "Epoch 20/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 98ms/step - accuracy: 0.9544 - loss: 0.1208 - val_accuracy: 0.5912 - val_loss: 1.5745\n",
      "Epoch 21/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 103ms/step - accuracy: 0.9542 - loss: 0.1140 - val_accuracy: 0.5611 - val_loss: 1.6157\n",
      "Epoch 22/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.9579 - loss: 0.1078 - val_accuracy: 0.5676 - val_loss: 1.6536\n",
      "Epoch 23/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9656 - loss: 0.0895 - val_accuracy: 0.5912 - val_loss: 1.5183\n",
      "Epoch 24/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9621 - loss: 0.1008 - val_accuracy: 0.5647 - val_loss: 1.8080\n",
      "Epoch 25/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9648 - loss: 0.0904 - val_accuracy: 0.5746 - val_loss: 1.9546\n",
      "Epoch 26/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9696 - loss: 0.0770 - val_accuracy: 0.5696 - val_loss: 1.9009\n",
      "Epoch 27/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.9717 - loss: 0.0789 - val_accuracy: 0.5733 - val_loss: 1.9446\n",
      "Epoch 28/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 99ms/step - accuracy: 0.9677 - loss: 0.0876 - val_accuracy: 0.5642 - val_loss: 1.8543\n",
      "Epoch 29/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.9689 - loss: 0.0826 - val_accuracy: 0.5839 - val_loss: 1.8787\n",
      "Epoch 30/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9679 - loss: 0.0814 - val_accuracy: 0.5603 - val_loss: 1.9281\n",
      "Epoch 31/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9725 - loss: 0.0715 - val_accuracy: 0.5798 - val_loss: 1.9919\n",
      "Epoch 32/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9702 - loss: 0.0784 - val_accuracy: 0.5857 - val_loss: 2.0216\n",
      "Epoch 33/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9685 - loss: 0.0773 - val_accuracy: 0.5543 - val_loss: 2.0092\n",
      "Epoch 34/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9707 - loss: 0.0710 - val_accuracy: 0.5735 - val_loss: 2.0262\n",
      "Epoch 35/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.9704 - loss: 0.0739 - val_accuracy: 0.5738 - val_loss: 1.9484\n",
      "Epoch 36/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.9732 - loss: 0.0657 - val_accuracy: 0.5585 - val_loss: 2.0570\n",
      "Epoch 37/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9722 - loss: 0.0706 - val_accuracy: 0.5608 - val_loss: 2.0829\n",
      "Epoch 38/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.9746 - loss: 0.0681 - val_accuracy: 0.5774 - val_loss: 2.3472\n",
      "Epoch 39/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9767 - loss: 0.0588 - val_accuracy: 0.5541 - val_loss: 2.1890\n",
      "Epoch 40/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.9751 - loss: 0.0647 - val_accuracy: 0.5772 - val_loss: 2.0521\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.6208 - loss: 1.7319\n",
      "Fold 1 validation accuracy: 0.5771725177764893\n",
      "Training fold 2...\n",
      "Epoch 1/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 102ms/step - accuracy: 0.6636 - loss: 0.9393 - val_accuracy: 0.6677 - val_loss: 0.6397\n",
      "Epoch 2/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.6664 - loss: 0.6378 - val_accuracy: 0.6677 - val_loss: 0.6378\n",
      "Epoch 3/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.6735 - loss: 0.6280 - val_accuracy: 0.6677 - val_loss: 0.6440\n",
      "Epoch 4/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 101ms/step - accuracy: 0.6735 - loss: 0.6156 - val_accuracy: 0.6677 - val_loss: 0.6489\n",
      "Epoch 5/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 100ms/step - accuracy: 0.6792 - loss: 0.5835 - val_accuracy: 0.6636 - val_loss: 0.6540\n",
      "Epoch 6/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.7157 - loss: 0.5437 - val_accuracy: 0.6176 - val_loss: 0.7166\n",
      "Epoch 7/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.7787 - loss: 0.4773 - val_accuracy: 0.5904 - val_loss: 0.7686\n",
      "Epoch 8/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.8221 - loss: 0.4053 - val_accuracy: 0.5870 - val_loss: 0.9336\n",
      "Epoch 9/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.8617 - loss: 0.3359 - val_accuracy: 0.5684 - val_loss: 0.9793\n",
      "Epoch 10/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.8830 - loss: 0.2831 - val_accuracy: 0.5507 - val_loss: 1.0639\n",
      "Epoch 11/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9148 - loss: 0.2316 - val_accuracy: 0.5746 - val_loss: 1.1052\n",
      "Epoch 12/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9242 - loss: 0.2030 - val_accuracy: 0.5632 - val_loss: 1.2717\n",
      "Epoch 13/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9349 - loss: 0.1788 - val_accuracy: 0.5564 - val_loss: 1.2859\n",
      "Epoch 14/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9422 - loss: 0.1612 - val_accuracy: 0.5891 - val_loss: 1.3246\n",
      "Epoch 15/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.9498 - loss: 0.1423 - val_accuracy: 0.5611 - val_loss: 1.4361\n",
      "Epoch 16/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9517 - loss: 0.1355 - val_accuracy: 0.5582 - val_loss: 1.6270\n",
      "Epoch 17/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 101ms/step - accuracy: 0.9537 - loss: 0.1300 - val_accuracy: 0.5447 - val_loss: 1.5668\n",
      "Epoch 18/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9534 - loss: 0.1280 - val_accuracy: 0.5681 - val_loss: 1.5886\n",
      "Epoch 19/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9593 - loss: 0.1139 - val_accuracy: 0.5730 - val_loss: 1.5783\n",
      "Epoch 20/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9610 - loss: 0.1110 - val_accuracy: 0.5621 - val_loss: 1.5399\n",
      "Epoch 21/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9651 - loss: 0.0980 - val_accuracy: 0.5590 - val_loss: 1.6322\n",
      "Epoch 22/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9620 - loss: 0.1060 - val_accuracy: 0.5715 - val_loss: 1.7146\n",
      "Epoch 23/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 103ms/step - accuracy: 0.9660 - loss: 0.0940 - val_accuracy: 0.5792 - val_loss: 1.9719\n",
      "Epoch 24/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9651 - loss: 0.0944 - val_accuracy: 0.5761 - val_loss: 1.8119\n",
      "Epoch 25/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 102ms/step - accuracy: 0.9700 - loss: 0.0801 - val_accuracy: 0.5655 - val_loss: 1.8186\n",
      "Epoch 26/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9703 - loss: 0.0821 - val_accuracy: 0.5689 - val_loss: 1.8397\n",
      "Epoch 27/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9708 - loss: 0.0805 - val_accuracy: 0.5608 - val_loss: 1.8989\n",
      "Epoch 28/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 103ms/step - accuracy: 0.9675 - loss: 0.0920 - val_accuracy: 0.5642 - val_loss: 1.8519\n",
      "Epoch 29/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9701 - loss: 0.0820 - val_accuracy: 0.5590 - val_loss: 1.9685\n",
      "Epoch 30/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9709 - loss: 0.0785 - val_accuracy: 0.5639 - val_loss: 1.8516\n",
      "Epoch 31/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9697 - loss: 0.0791 - val_accuracy: 0.5624 - val_loss: 2.1324\n",
      "Epoch 32/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9733 - loss: 0.0740 - val_accuracy: 0.5824 - val_loss: 1.9561\n",
      "Epoch 33/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9756 - loss: 0.0669 - val_accuracy: 0.5754 - val_loss: 1.8264\n",
      "Epoch 34/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.9715 - loss: 0.0751 - val_accuracy: 0.5696 - val_loss: 2.2072\n",
      "Epoch 35/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9743 - loss: 0.0724 - val_accuracy: 0.5790 - val_loss: 2.0531\n",
      "Epoch 36/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9764 - loss: 0.0665 - val_accuracy: 0.5790 - val_loss: 1.8858\n",
      "Epoch 37/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.9750 - loss: 0.0634 - val_accuracy: 0.5704 - val_loss: 2.2186\n",
      "Epoch 38/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.9740 - loss: 0.0708 - val_accuracy: 0.5842 - val_loss: 2.1615\n",
      "Epoch 39/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 103ms/step - accuracy: 0.9715 - loss: 0.0722 - val_accuracy: 0.5717 - val_loss: 1.7006\n",
      "Epoch 40/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.9539 - loss: 0.1223 - val_accuracy: 0.5681 - val_loss: 1.9781\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.5947 - loss: 1.7952\n",
      "Fold 2 validation accuracy: 0.5680933594703674\n",
      "Training fold 3...\n",
      "Epoch 1/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 106ms/step - accuracy: 0.6665 - loss: 0.9421 - val_accuracy: 0.6752 - val_loss: 0.6356\n",
      "Epoch 2/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.6723 - loss: 0.6347 - val_accuracy: 0.6752 - val_loss: 0.6373\n",
      "Epoch 3/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.6729 - loss: 0.6298 - val_accuracy: 0.6752 - val_loss: 0.6356\n",
      "Epoch 4/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.6728 - loss: 0.6171 - val_accuracy: 0.6752 - val_loss: 0.6463\n",
      "Epoch 5/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.6738 - loss: 0.5915 - val_accuracy: 0.6576 - val_loss: 0.6614\n",
      "Epoch 6/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 105ms/step - accuracy: 0.7231 - loss: 0.5453 - val_accuracy: 0.6428 - val_loss: 0.7513\n",
      "Epoch 7/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.7813 - loss: 0.4683 - val_accuracy: 0.6174 - val_loss: 0.8211\n",
      "Epoch 8/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.8249 - loss: 0.3949 - val_accuracy: 0.5668 - val_loss: 0.8815\n",
      "Epoch 9/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.8728 - loss: 0.3214 - val_accuracy: 0.5699 - val_loss: 0.9335\n",
      "Epoch 10/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.8931 - loss: 0.2690 - val_accuracy: 0.5891 - val_loss: 1.0167\n",
      "Epoch 11/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.9124 - loss: 0.2300 - val_accuracy: 0.5699 - val_loss: 1.2942\n",
      "Epoch 12/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 105ms/step - accuracy: 0.9227 - loss: 0.1982 - val_accuracy: 0.5782 - val_loss: 1.3052\n",
      "Epoch 13/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9303 - loss: 0.1778 - val_accuracy: 0.5878 - val_loss: 1.3281\n",
      "Epoch 14/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9425 - loss: 0.1523 - val_accuracy: 0.5837 - val_loss: 1.2842\n",
      "Epoch 15/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9465 - loss: 0.1432 - val_accuracy: 0.5523 - val_loss: 1.4498\n",
      "Epoch 16/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9486 - loss: 0.1386 - val_accuracy: 0.5748 - val_loss: 1.5362\n",
      "Epoch 17/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9513 - loss: 0.1293 - val_accuracy: 0.5626 - val_loss: 1.5497\n",
      "Epoch 18/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9526 - loss: 0.1282 - val_accuracy: 0.5756 - val_loss: 1.5786\n",
      "Epoch 19/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9579 - loss: 0.1073 - val_accuracy: 0.5790 - val_loss: 1.6775\n",
      "Epoch 20/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 105ms/step - accuracy: 0.9620 - loss: 0.1014 - val_accuracy: 0.5917 - val_loss: 1.7299\n",
      "Epoch 21/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9632 - loss: 0.1031 - val_accuracy: 0.5668 - val_loss: 1.5768\n",
      "Epoch 22/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9603 - loss: 0.0982 - val_accuracy: 0.5824 - val_loss: 1.8135\n",
      "Epoch 23/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9644 - loss: 0.0959 - val_accuracy: 0.5754 - val_loss: 1.6828\n",
      "Epoch 24/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9642 - loss: 0.0974 - val_accuracy: 0.5645 - val_loss: 1.8320\n",
      "Epoch 25/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9658 - loss: 0.0868 - val_accuracy: 0.5875 - val_loss: 1.7986\n",
      "Epoch 26/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9673 - loss: 0.0849 - val_accuracy: 0.5863 - val_loss: 1.8802\n",
      "Epoch 27/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9651 - loss: 0.0857 - val_accuracy: 0.5652 - val_loss: 1.8468\n",
      "Epoch 28/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9703 - loss: 0.0770 - val_accuracy: 0.5733 - val_loss: 2.0232\n",
      "Epoch 29/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9700 - loss: 0.0793 - val_accuracy: 0.5738 - val_loss: 1.7199\n",
      "Epoch 30/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9656 - loss: 0.0850 - val_accuracy: 0.5725 - val_loss: 1.8644\n",
      "Epoch 31/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9717 - loss: 0.0740 - val_accuracy: 0.5855 - val_loss: 1.9885\n",
      "Epoch 32/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9677 - loss: 0.0820 - val_accuracy: 0.5715 - val_loss: 1.8831\n",
      "Epoch 33/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9728 - loss: 0.0663 - val_accuracy: 0.5790 - val_loss: 1.9418\n",
      "Epoch 34/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9713 - loss: 0.0706 - val_accuracy: 0.5818 - val_loss: 1.9495\n",
      "Epoch 35/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9735 - loss: 0.0702 - val_accuracy: 0.5730 - val_loss: 1.8918\n",
      "Epoch 36/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9699 - loss: 0.0722 - val_accuracy: 0.5751 - val_loss: 2.0874\n",
      "Epoch 37/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9741 - loss: 0.0674 - val_accuracy: 0.5751 - val_loss: 1.9390\n",
      "Epoch 38/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9733 - loss: 0.0666 - val_accuracy: 0.5611 - val_loss: 1.8750\n",
      "Epoch 39/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9743 - loss: 0.0656 - val_accuracy: 0.5730 - val_loss: 1.8845\n",
      "Epoch 40/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9730 - loss: 0.0655 - val_accuracy: 0.5842 - val_loss: 1.8224\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.6157 - loss: 1.5854\n",
      "Fold 3 validation accuracy: 0.5841764211654663\n",
      "Training fold 4...\n",
      "Epoch 1/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 108ms/step - accuracy: 0.6736 - loss: 0.9434 - val_accuracy: 0.6763 - val_loss: 0.6347\n",
      "Epoch 2/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.6705 - loss: 0.6357 - val_accuracy: 0.6763 - val_loss: 0.6360\n",
      "Epoch 3/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.6716 - loss: 0.6296 - val_accuracy: 0.6763 - val_loss: 0.6357\n",
      "Epoch 4/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.6679 - loss: 0.6205 - val_accuracy: 0.6763 - val_loss: 0.6456\n",
      "Epoch 5/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.6809 - loss: 0.5864 - val_accuracy: 0.6501 - val_loss: 0.6676\n",
      "Epoch 6/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.7296 - loss: 0.5415 - val_accuracy: 0.6407 - val_loss: 0.6911\n",
      "Epoch 7/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.7707 - loss: 0.4829 - val_accuracy: 0.5818 - val_loss: 0.7941\n",
      "Epoch 8/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.8199 - loss: 0.4078 - val_accuracy: 0.5660 - val_loss: 0.7964\n",
      "Epoch 9/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.8641 - loss: 0.3376 - val_accuracy: 0.5894 - val_loss: 1.0581\n",
      "Epoch 10/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.8846 - loss: 0.2857 - val_accuracy: 0.5914 - val_loss: 1.0454\n",
      "Epoch 11/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9074 - loss: 0.2367 - val_accuracy: 0.5795 - val_loss: 1.1545\n",
      "Epoch 12/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9212 - loss: 0.2082 - val_accuracy: 0.5741 - val_loss: 1.2170\n",
      "Epoch 13/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9253 - loss: 0.1917 - val_accuracy: 0.5608 - val_loss: 1.2626\n",
      "Epoch 14/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9339 - loss: 0.1732 - val_accuracy: 0.5717 - val_loss: 1.4049\n",
      "Epoch 15/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9452 - loss: 0.1476 - val_accuracy: 0.5722 - val_loss: 1.4354\n",
      "Epoch 16/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9503 - loss: 0.1319 - val_accuracy: 0.5712 - val_loss: 1.4472\n",
      "Epoch 17/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9509 - loss: 0.1302 - val_accuracy: 0.5751 - val_loss: 1.6315\n",
      "Epoch 18/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9560 - loss: 0.1232 - val_accuracy: 0.5860 - val_loss: 1.4785\n",
      "Epoch 19/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9579 - loss: 0.1146 - val_accuracy: 0.5712 - val_loss: 1.7049\n",
      "Epoch 20/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.9562 - loss: 0.1138 - val_accuracy: 0.5681 - val_loss: 1.8185\n",
      "Epoch 21/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9603 - loss: 0.1060 - val_accuracy: 0.5754 - val_loss: 1.6672\n",
      "Epoch 22/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9578 - loss: 0.1110 - val_accuracy: 0.5730 - val_loss: 1.7270\n",
      "Epoch 23/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9651 - loss: 0.0918 - val_accuracy: 0.5790 - val_loss: 1.8133\n",
      "Epoch 24/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9629 - loss: 0.0938 - val_accuracy: 0.5831 - val_loss: 1.8026\n",
      "Epoch 25/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9639 - loss: 0.0905 - val_accuracy: 0.5686 - val_loss: 1.6996\n",
      "Epoch 26/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9633 - loss: 0.0964 - val_accuracy: 0.5694 - val_loss: 1.9140\n",
      "Epoch 27/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9650 - loss: 0.0931 - val_accuracy: 0.5699 - val_loss: 1.7473\n",
      "Epoch 28/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9679 - loss: 0.0860 - val_accuracy: 0.5645 - val_loss: 1.7914\n",
      "Epoch 29/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9695 - loss: 0.0820 - val_accuracy: 0.5795 - val_loss: 1.9656\n",
      "Epoch 30/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9721 - loss: 0.0727 - val_accuracy: 0.5777 - val_loss: 2.0112\n",
      "Epoch 31/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9681 - loss: 0.0842 - val_accuracy: 0.5767 - val_loss: 1.9746\n",
      "Epoch 32/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.9669 - loss: 0.0884 - val_accuracy: 0.5671 - val_loss: 1.9482\n",
      "Epoch 33/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9713 - loss: 0.0742 - val_accuracy: 0.5881 - val_loss: 2.0350\n",
      "Epoch 34/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9633 - loss: 0.0950 - val_accuracy: 0.5847 - val_loss: 1.8925\n",
      "Epoch 35/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9704 - loss: 0.0764 - val_accuracy: 0.5717 - val_loss: 2.0053\n",
      "Epoch 36/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9750 - loss: 0.0639 - val_accuracy: 0.5795 - val_loss: 2.2038\n",
      "Epoch 37/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9800 - loss: 0.0546 - val_accuracy: 0.5748 - val_loss: 2.0496\n",
      "Epoch 38/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9752 - loss: 0.0610 - val_accuracy: 0.5816 - val_loss: 2.2249\n",
      "Epoch 39/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9729 - loss: 0.0699 - val_accuracy: 0.5792 - val_loss: 2.1600\n",
      "Epoch 40/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9749 - loss: 0.0640 - val_accuracy: 0.5743 - val_loss: 2.0286\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.6067 - loss: 1.7862\n",
      "Fold 4 validation accuracy: 0.574319064617157\n",
      "Training fold 5...\n",
      "Epoch 1/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 109ms/step - accuracy: 0.6736 - loss: 0.9385 - val_accuracy: 0.6651 - val_loss: 0.6403\n",
      "Epoch 2/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.6762 - loss: 0.6309 - val_accuracy: 0.6651 - val_loss: 0.6396\n",
      "Epoch 3/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.6750 - loss: 0.6252 - val_accuracy: 0.6651 - val_loss: 0.6504\n",
      "Epoch 4/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.6781 - loss: 0.6119 - val_accuracy: 0.6651 - val_loss: 0.6531\n",
      "Epoch 5/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.6775 - loss: 0.5814 - val_accuracy: 0.6560 - val_loss: 0.6731\n",
      "Epoch 6/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.7206 - loss: 0.5449 - val_accuracy: 0.6080 - val_loss: 0.6871\n",
      "Epoch 7/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.7765 - loss: 0.4787 - val_accuracy: 0.6096 - val_loss: 0.7642\n",
      "Epoch 8/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.8264 - loss: 0.4009 - val_accuracy: 0.5785 - val_loss: 0.8772\n",
      "Epoch 9/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.8626 - loss: 0.3343 - val_accuracy: 0.5678 - val_loss: 1.0158\n",
      "Epoch 10/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.8851 - loss: 0.2779 - val_accuracy: 0.5632 - val_loss: 1.0669\n",
      "Epoch 11/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9081 - loss: 0.2396 - val_accuracy: 0.5728 - val_loss: 1.1402\n",
      "Epoch 12/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9220 - loss: 0.2077 - val_accuracy: 0.5780 - val_loss: 1.2281\n",
      "Epoch 13/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9281 - loss: 0.1853 - val_accuracy: 0.5813 - val_loss: 1.3741\n",
      "Epoch 14/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9395 - loss: 0.1641 - val_accuracy: 0.5782 - val_loss: 1.3982\n",
      "Epoch 15/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9483 - loss: 0.1407 - val_accuracy: 0.5699 - val_loss: 1.5405\n",
      "Epoch 16/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9478 - loss: 0.1367 - val_accuracy: 0.5720 - val_loss: 1.5519\n",
      "Epoch 17/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9496 - loss: 0.1340 - val_accuracy: 0.5595 - val_loss: 1.5803\n",
      "Epoch 18/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9521 - loss: 0.1266 - val_accuracy: 0.5580 - val_loss: 1.6683\n",
      "Epoch 19/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9576 - loss: 0.1122 - val_accuracy: 0.5652 - val_loss: 1.6606\n",
      "Epoch 20/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9566 - loss: 0.1182 - val_accuracy: 0.5619 - val_loss: 1.7527\n",
      "Epoch 21/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9618 - loss: 0.1042 - val_accuracy: 0.5665 - val_loss: 1.6806\n",
      "Epoch 22/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9643 - loss: 0.0927 - val_accuracy: 0.5761 - val_loss: 1.7783\n",
      "Epoch 23/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.9608 - loss: 0.1014 - val_accuracy: 0.5668 - val_loss: 1.7334\n",
      "Epoch 24/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 111ms/step - accuracy: 0.9644 - loss: 0.0959 - val_accuracy: 0.5616 - val_loss: 1.7114\n",
      "Epoch 25/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9622 - loss: 0.1032 - val_accuracy: 0.5712 - val_loss: 1.8840\n",
      "Epoch 26/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9661 - loss: 0.0928 - val_accuracy: 0.5772 - val_loss: 1.8155\n",
      "Epoch 27/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9665 - loss: 0.0839 - val_accuracy: 0.5502 - val_loss: 1.8759\n",
      "Epoch 28/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9625 - loss: 0.0950 - val_accuracy: 0.5575 - val_loss: 1.8570\n",
      "Epoch 29/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9683 - loss: 0.0846 - val_accuracy: 0.5699 - val_loss: 1.9220\n",
      "Epoch 30/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9700 - loss: 0.0776 - val_accuracy: 0.5733 - val_loss: 1.9566\n",
      "Epoch 31/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9726 - loss: 0.0732 - val_accuracy: 0.5518 - val_loss: 2.0093\n",
      "Epoch 32/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9700 - loss: 0.0721 - val_accuracy: 0.5554 - val_loss: 2.0199\n",
      "Epoch 33/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9701 - loss: 0.0776 - val_accuracy: 0.5769 - val_loss: 1.9115\n",
      "Epoch 34/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9729 - loss: 0.0697 - val_accuracy: 0.5637 - val_loss: 2.0221\n",
      "Epoch 35/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9712 - loss: 0.0786 - val_accuracy: 0.5671 - val_loss: 2.0306\n",
      "Epoch 36/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.9724 - loss: 0.0749 - val_accuracy: 0.5761 - val_loss: 1.9226\n",
      "Epoch 37/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9742 - loss: 0.0657 - val_accuracy: 0.5642 - val_loss: 2.0976\n",
      "Epoch 38/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9715 - loss: 0.0744 - val_accuracy: 0.5699 - val_loss: 2.0030\n",
      "Epoch 39/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.9741 - loss: 0.0667 - val_accuracy: 0.5523 - val_loss: 1.9463\n",
      "Epoch 40/40\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.9761 - loss: 0.0692 - val_accuracy: 0.5652 - val_loss: 2.0787\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.6066 - loss: 1.8743\n",
      "Fold 5 validation accuracy: 0.5652399659156799\n",
      "Cross-validation completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assuming you have tokenized your data and have an embedding matrix\n",
    "# tokenizer, max_seq_length, embedding_matrix_fasttext\n",
    "\n",
    "# Prepare your data\n",
    "X = np.array(X_augmented)  # Assuming X_augmented is your augmented dataset\n",
    "y = np.array(y_train)      # Assuming y_train is your training labels\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Create a new instance of the model for each fold\n",
    "    model = create_lstm_model_with_regularization(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the training data of the current fold\n",
    "    print(f\"Training fold {fold_no}...\")\n",
    "    history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=40, batch_size=32)\n",
    "    \n",
    "    # Evaluate the model on the validation data of the current fold\n",
    "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold)\n",
    "    print(f\"Fold {fold_no} validation accuracy: {accuracy}\")\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "65dfc113-58e7-4f4c-a743-20bd4b562197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmedma\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Epoch 1/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 107ms/step - accuracy: 0.6424 - loss: 1.0228 - val_accuracy: 0.6773 - val_loss: 0.6450\n",
      "Epoch 2/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.6689 - loss: 0.6447 - val_accuracy: 0.6773 - val_loss: 0.6370\n",
      "Epoch 3/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.6695 - loss: 0.6367 - val_accuracy: 0.6773 - val_loss: 0.6341\n",
      "Epoch 4/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.6688 - loss: 0.6314 - val_accuracy: 0.6773 - val_loss: 0.6353\n",
      "Epoch 5/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.6723 - loss: 0.6252 - val_accuracy: 0.6773 - val_loss: 0.6459\n",
      "Epoch 6/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.6677 - loss: 0.6182 - val_accuracy: 0.6773 - val_loss: 0.6576\n",
      "Epoch 7/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.6697 - loss: 0.6062 - val_accuracy: 0.6773 - val_loss: 0.6574\n",
      "Epoch 8/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.6817 - loss: 0.5854 - val_accuracy: 0.5951 - val_loss: 0.6775\n",
      "Epoch 9/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.7046 - loss: 0.5656 - val_accuracy: 0.6140 - val_loss: 0.7078\n",
      "Epoch 10/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.7299 - loss: 0.5377 - val_accuracy: 0.5974 - val_loss: 0.7289\n",
      "Epoch 11/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.7531 - loss: 0.5031 - val_accuracy: 0.5860 - val_loss: 0.7789\n",
      "Epoch 12/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.7911 - loss: 0.4544 - val_accuracy: 0.5256 - val_loss: 0.8553\n",
      "Epoch 13/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.8035 - loss: 0.4208 - val_accuracy: 0.5647 - val_loss: 0.8629\n",
      "Epoch 14/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.8396 - loss: 0.3708 - val_accuracy: 0.5907 - val_loss: 0.9134\n",
      "Epoch 15/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.8568 - loss: 0.3384 - val_accuracy: 0.5499 - val_loss: 1.0345\n",
      "Epoch 16/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 104ms/step - accuracy: 0.8714 - loss: 0.3127 - val_accuracy: 0.5997 - val_loss: 1.0931\n",
      "Epoch 17/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.8876 - loss: 0.2775 - val_accuracy: 0.5616 - val_loss: 1.1433\n",
      "Epoch 18/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.8970 - loss: 0.2543 - val_accuracy: 0.5613 - val_loss: 1.3261\n",
      "Epoch 19/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9097 - loss: 0.2325 - val_accuracy: 0.6073 - val_loss: 1.4061\n",
      "Epoch 20/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9090 - loss: 0.2228 - val_accuracy: 0.5613 - val_loss: 1.3325\n",
      "Epoch 21/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9230 - loss: 0.2011 - val_accuracy: 0.5912 - val_loss: 1.4360\n",
      "Epoch 22/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9332 - loss: 0.1815 - val_accuracy: 0.6034 - val_loss: 1.4388\n",
      "Epoch 23/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9358 - loss: 0.1667 - val_accuracy: 0.5875 - val_loss: 1.4197\n",
      "Epoch 24/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9359 - loss: 0.1618 - val_accuracy: 0.5850 - val_loss: 1.5329\n",
      "Epoch 25/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 105ms/step - accuracy: 0.9425 - loss: 0.1519 - val_accuracy: 0.5767 - val_loss: 1.5946\n",
      "Epoch 26/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9399 - loss: 0.1545 - val_accuracy: 0.5865 - val_loss: 1.6705\n",
      "Epoch 27/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9387 - loss: 0.1599 - val_accuracy: 0.5748 - val_loss: 1.4993\n",
      "Epoch 28/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9437 - loss: 0.1493 - val_accuracy: 0.5792 - val_loss: 1.7756\n",
      "Epoch 29/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9522 - loss: 0.1287 - val_accuracy: 0.5821 - val_loss: 1.6235\n",
      "Epoch 30/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9454 - loss: 0.1394 - val_accuracy: 0.5881 - val_loss: 1.6524\n",
      "Epoch 31/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9515 - loss: 0.1302 - val_accuracy: 0.5681 - val_loss: 1.7366\n",
      "Epoch 32/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 107ms/step - accuracy: 0.9534 - loss: 0.1184 - val_accuracy: 0.5927 - val_loss: 1.8391\n",
      "Epoch 33/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9607 - loss: 0.1036 - val_accuracy: 0.5725 - val_loss: 1.8089\n",
      "Epoch 34/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9566 - loss: 0.1196 - val_accuracy: 0.5907 - val_loss: 1.8232\n",
      "Epoch 35/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9554 - loss: 0.1239 - val_accuracy: 0.5852 - val_loss: 1.5694\n",
      "Epoch 36/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9576 - loss: 0.1157 - val_accuracy: 0.5914 - val_loss: 1.8494\n",
      "Epoch 37/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9595 - loss: 0.1108 - val_accuracy: 0.5580 - val_loss: 1.8481\n",
      "Epoch 38/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9641 - loss: 0.0970 - val_accuracy: 0.5943 - val_loss: 1.8915\n",
      "Epoch 39/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 111ms/step - accuracy: 0.9636 - loss: 0.0988 - val_accuracy: 0.5888 - val_loss: 1.8979\n",
      "Epoch 40/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9602 - loss: 0.1041 - val_accuracy: 0.5754 - val_loss: 2.0081\n",
      "Epoch 41/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 113ms/step - accuracy: 0.9584 - loss: 0.1141 - val_accuracy: 0.5787 - val_loss: 1.7899\n",
      "Epoch 42/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 112ms/step - accuracy: 0.9609 - loss: 0.1042 - val_accuracy: 0.5671 - val_loss: 2.0969\n",
      "Epoch 43/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9718 - loss: 0.0836 - val_accuracy: 0.5847 - val_loss: 1.9107\n",
      "Epoch 44/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 116ms/step - accuracy: 0.9693 - loss: 0.0933 - val_accuracy: 0.5813 - val_loss: 2.0356\n",
      "Epoch 45/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 110ms/step - accuracy: 0.9675 - loss: 0.0917 - val_accuracy: 0.5601 - val_loss: 1.8823\n",
      "Epoch 46/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.9640 - loss: 0.1021 - val_accuracy: 0.5894 - val_loss: 1.8660\n",
      "Epoch 47/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 108ms/step - accuracy: 0.9716 - loss: 0.0861 - val_accuracy: 0.5756 - val_loss: 2.0993\n",
      "Epoch 48/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 107ms/step - accuracy: 0.9756 - loss: 0.0758 - val_accuracy: 0.5720 - val_loss: 1.7329\n",
      "Epoch 49/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 106ms/step - accuracy: 0.9711 - loss: 0.0846 - val_accuracy: 0.5603 - val_loss: 2.0443\n",
      "Epoch 50/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.9707 - loss: 0.0861 - val_accuracy: 0.5720 - val_loss: 1.8795\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.6026 - loss: 1.7242\n",
      "Fold 1 validation accuracy: 0.5719844102859497\n",
      "Training fold 2...\n",
      "Epoch 1/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 112ms/step - accuracy: 0.6501 - loss: 1.0205 - val_accuracy: 0.6677 - val_loss: 0.6538\n",
      "Epoch 2/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.6665 - loss: 0.6449 - val_accuracy: 0.6677 - val_loss: 0.6467\n",
      "Epoch 3/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.6716 - loss: 0.6344 - val_accuracy: 0.6677 - val_loss: 0.6415\n",
      "Epoch 4/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 111ms/step - accuracy: 0.6735 - loss: 0.6295 - val_accuracy: 0.6677 - val_loss: 0.6403\n",
      "Epoch 5/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 111ms/step - accuracy: 0.6679 - loss: 0.6264 - val_accuracy: 0.6677 - val_loss: 0.6443\n",
      "Epoch 6/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.6816 - loss: 0.6058 - val_accuracy: 0.6677 - val_loss: 0.6569\n",
      "Epoch 7/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.6760 - loss: 0.5932 - val_accuracy: 0.6677 - val_loss: 0.6681\n",
      "Epoch 8/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.6785 - loss: 0.5784 - val_accuracy: 0.6309 - val_loss: 0.6942\n",
      "Epoch 9/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.7202 - loss: 0.5483 - val_accuracy: 0.5479 - val_loss: 0.7391\n",
      "Epoch 10/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.7476 - loss: 0.5163 - val_accuracy: 0.5673 - val_loss: 0.8136\n",
      "Epoch 11/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.7774 - loss: 0.4760 - val_accuracy: 0.5743 - val_loss: 0.8235\n",
      "Epoch 12/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.8091 - loss: 0.4249 - val_accuracy: 0.5759 - val_loss: 0.8954\n",
      "Epoch 13/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 113ms/step - accuracy: 0.8167 - loss: 0.4140 - val_accuracy: 0.5839 - val_loss: 0.9682\n",
      "Epoch 14/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 109ms/step - accuracy: 0.8425 - loss: 0.3601 - val_accuracy: 0.5772 - val_loss: 1.0244\n",
      "Epoch 15/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.8616 - loss: 0.3232 - val_accuracy: 0.5676 - val_loss: 1.1182\n",
      "Epoch 16/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 109ms/step - accuracy: 0.8747 - loss: 0.3008 - val_accuracy: 0.5549 - val_loss: 1.1679\n",
      "Epoch 17/50\n",
      "\u001b[1m427/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.8958 - loss: 0.2628"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[230], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model on the training data of the current fold\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_fold, y_train_fold, validation_data\u001b[38;5;241m=\u001b[39m(X_val_fold, y_val_fold), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation data of the current fold\u001b[39;00m\n\u001b[0;32m     47\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val_fold, y_val_fold)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model architecture with dropout, batch normalization, and weight regularization\n",
    "def create_lstm_model_with_regularization(vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                        output_dim=embedding_dim,\n",
    "                                        weights=[embedding_matrix],\n",
    "                                        input_length=max_seq_length,\n",
    "                                        trainable=False))\n",
    "    model.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))  # Increase dropout rate\n",
    "    model.add(BatchNormalization())  # Add batch normalization\n",
    "    model.add(tf.keras.layers.LSTM(128))\n",
    "    model.add(Dropout(0.5))  # Increase dropout rate\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(0.01), activation='relu'))  # Apply weight regularization\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Prepare your data\n",
    "X = np.array(X_augmented)  # Assuming X_augmented is your augmented dataset\n",
    "y = np.array(y_train)      # Assuming y_train is your training labels\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "fold_no = 1\n",
    "val_accuracies = []\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Create a new instance of the model for each fold\n",
    "    model = create_lstm_model_with_regularization(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the training data of the current fold\n",
    "    print(f\"Training fold {fold_no}...\")\n",
    "    history = model.fit(X_train_fold, y_train_fold, validation_data=(X_val_fold, y_val_fold), epochs=50, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Evaluate the model on the validation data of the current fold\n",
    "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold)\n",
    "    print(f\"Fold {fold_no} validation accuracy: {accuracy}\")\n",
    "    val_accuracies.append(accuracy)\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation completed.\")\n",
    "print(f\"Average validation accuracy: {np.mean(val_accuracies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dd6b4070-23b9-455b-ae5c-9ce4ff29eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Epoch 1/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 56ms/step - accuracy: 0.6393 - loss: 0.8814 - val_accuracy: 0.6773 - val_loss: 0.6386 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6734 - loss: 0.6371 - val_accuracy: 0.6773 - val_loss: 0.6314 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.6743 - loss: 0.6316 - val_accuracy: 0.6773 - val_loss: 0.6323 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.6681 - loss: 0.6340 - val_accuracy: 0.6773 - val_loss: 0.6332 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.6602 - loss: 0.6371 - val_accuracy: 0.6773 - val_loss: 0.6343 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6681 - loss: 0.6273 - val_accuracy: 0.6773 - val_loss: 0.6382 - learning_rate: 2.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.6709 - loss: 0.6184 - val_accuracy: 0.6773 - val_loss: 0.6421 - learning_rate: 2.0000e-04\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7550 - loss: 0.5755\n",
      "Fold 1 validation accuracy: 0.6773021817207336, validation loss: 0.6314487457275391\n",
      "Training fold 2...\n",
      "Epoch 1/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 55ms/step - accuracy: 0.6430 - loss: 0.8692 - val_accuracy: 0.6677 - val_loss: 0.6431 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.6692 - loss: 0.6391 - val_accuracy: 0.6677 - val_loss: 0.6393 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.6727 - loss: 0.6336 - val_accuracy: 0.6677 - val_loss: 0.6442 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 54ms/step - accuracy: 0.6755 - loss: 0.6301 - val_accuracy: 0.6677 - val_loss: 0.6411 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 55ms/step - accuracy: 0.6708 - loss: 0.6307 - val_accuracy: 0.6677 - val_loss: 0.6395 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6770 - loss: 0.6224 - val_accuracy: 0.6677 - val_loss: 0.6446 - learning_rate: 2.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6744 - loss: 0.6191 - val_accuracy: 0.6677 - val_loss: 0.6517 - learning_rate: 2.0000e-04\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7439 - loss: 0.5832\n",
      "Fold 2 validation accuracy: 0.6677042841911316, validation loss: 0.6393098831176758\n",
      "Training fold 3...\n",
      "Epoch 1/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 58ms/step - accuracy: 0.6683 - loss: 0.8845 - val_accuracy: 0.6752 - val_loss: 0.6447 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.6699 - loss: 0.6440 - val_accuracy: 0.6752 - val_loss: 0.6361 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6754 - loss: 0.6326 - val_accuracy: 0.6752 - val_loss: 0.6340 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6778 - loss: 0.6279 - val_accuracy: 0.6752 - val_loss: 0.6348 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6679 - loss: 0.6318 - val_accuracy: 0.6752 - val_loss: 0.6337 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.6759 - loss: 0.6250 - val_accuracy: 0.6752 - val_loss: 0.6379 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.6725 - loss: 0.6247 - val_accuracy: 0.6752 - val_loss: 0.6378 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6709 - loss: 0.6236 - val_accuracy: 0.6752 - val_loss: 0.6411 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.6745 - loss: 0.6096 - val_accuracy: 0.6752 - val_loss: 0.6523 - learning_rate: 2.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 55ms/step - accuracy: 0.6717 - loss: 0.6019 - val_accuracy: 0.6752 - val_loss: 0.6589 - learning_rate: 2.0000e-04\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.7566 - loss: 0.5842\n",
      "Fold 3 validation accuracy: 0.6752269864082336, validation loss: 0.633690595626831\n",
      "Training fold 4...\n",
      "Epoch 1/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 59ms/step - accuracy: 0.6508 - loss: 0.8665 - val_accuracy: 0.6763 - val_loss: 0.6384 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 57ms/step - accuracy: 0.6809 - loss: 0.6309 - val_accuracy: 0.6763 - val_loss: 0.6342 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.6767 - loss: 0.6314 - val_accuracy: 0.6763 - val_loss: 0.6344 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 57ms/step - accuracy: 0.6690 - loss: 0.6350 - val_accuracy: 0.6763 - val_loss: 0.6333 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.6724 - loss: 0.6312 - val_accuracy: 0.6763 - val_loss: 0.6369 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.6762 - loss: 0.6252 - val_accuracy: 0.6763 - val_loss: 0.6381 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 58ms/step - accuracy: 0.6696 - loss: 0.6266 - val_accuracy: 0.6763 - val_loss: 0.6415 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 64ms/step - accuracy: 0.6698 - loss: 0.6161 - val_accuracy: 0.6763 - val_loss: 0.6458 - learning_rate: 2.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 63ms/step - accuracy: 0.6686 - loss: 0.6085 - val_accuracy: 0.6763 - val_loss: 0.6464 - learning_rate: 2.0000e-04\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7510 - loss: 0.5852\n",
      "Fold 4 validation accuracy: 0.6762645840644836, validation loss: 0.6333115100860596\n",
      "Training fold 5...\n",
      "Epoch 1/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 64ms/step - accuracy: 0.6481 - loss: 0.8797 - val_accuracy: 0.6651 - val_loss: 0.6540 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 62ms/step - accuracy: 0.6711 - loss: 0.6397 - val_accuracy: 0.6651 - val_loss: 0.6421 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.6665 - loss: 0.6367 - val_accuracy: 0.6651 - val_loss: 0.6438 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 63ms/step - accuracy: 0.6736 - loss: 0.6311 - val_accuracy: 0.6651 - val_loss: 0.6422 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 60ms/step - accuracy: 0.6814 - loss: 0.6221 - val_accuracy: 0.6651 - val_loss: 0.6454 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 61ms/step - accuracy: 0.6699 - loss: 0.6259 - val_accuracy: 0.6651 - val_loss: 0.6498 - learning_rate: 2.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 61ms/step - accuracy: 0.6724 - loss: 0.6203 - val_accuracy: 0.6651 - val_loss: 0.6540 - learning_rate: 2.0000e-04\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.7410 - loss: 0.5993\n",
      "Fold 5 validation accuracy: 0.6651102304458618, validation loss: 0.642104983329773\n",
      "Cross-validation completed.\n",
      "Average validation accuracy: 0.6723216533660888\n",
      "Average validation loss: 0.6359731435775757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization, LSTM, Embedding, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the Attention layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.dot(x, self.W) + self.b\n",
    "        e = tf.keras.backend.tanh(e)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# Define the model architecture with dropout, batch normalization, and weight regularization\n",
    "def create_lstm_model_with_regularization(vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_seq_length,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))  # Adjust dropout rate\n",
    "    model.add(BatchNormalization())  # Add batch normalization\n",
    "    model.add(Attention())\n",
    "    model.add(Dense(32, kernel_regularizer=regularizers.l2(0.01), activation='relu'))  # Reduce number of units\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Prepare your data\n",
    "X = np.array(X_augmented)  # Assuming X_augmented is your augmented dataset\n",
    "y = np.array(y_train)      # Assuming y_train is your training labels\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "fold_no = 1\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Create a new instance of the model for each fold\n",
    "    model = create_lstm_model_with_regularization(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks for early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "    \n",
    "    # Train the model on the training data of the current fold\n",
    "    print(f\"Training fold {fold_no}...\")\n",
    "    history = model.fit(X_train_fold, y_train_fold,\n",
    "                        validation_data=(X_val_fold, y_val_fold),\n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Evaluate the model on the validation data of the current fold\n",
    "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold)\n",
    "    print(f\"Fold {fold_no} validation accuracy: {accuracy}, validation loss: {loss}\")\n",
    "    val_accuracies.append(accuracy)\n",
    "    val_losses.append(loss)\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation completed.\")\n",
    "print(f\"Average validation accuracy: {np.mean(val_accuracies)}\")\n",
    "print(f\"Average validation loss: {np.mean(val_losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad4a8db-a60d-4ecf-b3a7-8762ddecda23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2b3f133f-fdf2-474c-b3fe-4e6a4023311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n",
      "Epoch 1/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 570ms/step - accuracy: 0.6689 - loss: 1.3283 - val_accuracy: 0.6773 - val_loss: 0.6330 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 577ms/step - accuracy: 0.6779 - loss: 0.6327 - val_accuracy: 0.6773 - val_loss: 0.6362 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 592ms/step - accuracy: 0.6725 - loss: 0.6337 - val_accuracy: 0.6773 - val_loss: 0.6319 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 576ms/step - accuracy: 0.6715 - loss: 0.6342 - val_accuracy: 0.6773 - val_loss: 0.6317 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 592ms/step - accuracy: 0.6734 - loss: 0.6299 - val_accuracy: 0.6773 - val_loss: 0.6352 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m 19/241\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 531ms/step - accuracy: 0.6560 - loss: 0.6373"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[233], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Train the model on the training data of the current fold\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train_fold, y_train_fold,\n\u001b[0;32m     72\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(X_val_fold, y_val_fold),\n\u001b[0;32m     73\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# Increase epochs\u001b[39;00m\n\u001b[0;32m     74\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,  \u001b[38;5;66;03m# Increase batch size\u001b[39;00m\n\u001b[0;32m     75\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, reduce_lr],\n\u001b[0;32m     76\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation data of the current fold\u001b[39;00m\n\u001b[0;32m     79\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val_fold, y_val_fold)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization, LSTM, Embedding, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the Attention layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.dot(x, self.W) + self.b\n",
    "        e = tf.keras.backend.tanh(e)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# Define the model architecture with dropout, batch normalization, and weight regularization\n",
    "def create_lstm_model_with_regularization(vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_seq_length,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Increased LSTM units\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Added another LSTM layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Attention())\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.01), activation='relu'))  # Increased Dense layer units\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Prepare your data\n",
    "X = np.array(X_augmented)  # Assuming X_augmented is your augmented dataset\n",
    "y = np.array(y_train)      # Assuming y_train is your training labels\n",
    "\n",
    "# Define k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "fold_no = 1\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Create a new instance of the model for each fold\n",
    "    model = create_lstm_model_with_regularization(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "    optimizer = Adam(learning_rate=0.001)  # Use Adam optimizer with learning rate\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks for early stopping and learning rate reduction\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "    \n",
    "    # Train the model on the training data of the current fold\n",
    "    print(f\"Training fold {fold_no}...\")\n",
    "    history = model.fit(X_train_fold, y_train_fold,\n",
    "                        validation_data=(X_val_fold, y_val_fold),\n",
    "                        epochs=100,  # Increase epochs\n",
    "                        batch_size=64,  # Increase batch size\n",
    "                        callbacks=[early_stopping, reduce_lr],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Evaluate the model on the validation data of the current fold\n",
    "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold)\n",
    "    print(f\"Fold {fold_no} validation accuracy: {accuracy}, validation loss: {loss}\")\n",
    "    val_accuracies.append(accuracy)\n",
    "    val_losses.append(loss)\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation completed.\")\n",
    "print(f\"Average validation accuracy: {np.mean(val_accuracies)}\")\n",
    "print(f\"Average validation loss: {np.mean(val_losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "03970476-cf1a-4af2-9783-7bb5cfda24c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 83ms/step - accuracy: 0.8937 - loss: 0.7690 - val_accuracy: 0.9607 - val_loss: 0.1459 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 80ms/step - accuracy: 0.9537 - loss: 0.1540 - val_accuracy: 0.9417 - val_loss: 0.1657 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 81ms/step - accuracy: 0.9683 - loss: 0.1057 - val_accuracy: 0.9732 - val_loss: 0.0947 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 82ms/step - accuracy: 0.9730 - loss: 0.0870 - val_accuracy: 0.9739 - val_loss: 0.1008 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 80ms/step - accuracy: 0.9773 - loss: 0.0760 - val_accuracy: 0.9719 - val_loss: 0.0969 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 81ms/step - accuracy: 0.9824 - loss: 0.0596 - val_accuracy: 0.9782 - val_loss: 0.0772 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 85ms/step - accuracy: 0.9889 - loss: 0.0468 - val_accuracy: 0.9778 - val_loss: 0.0809 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 80ms/step - accuracy: 0.9888 - loss: 0.0438 - val_accuracy: 0.9778 - val_loss: 0.0849 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 80ms/step - accuracy: 0.9910 - loss: 0.0369 - val_accuracy: 0.9778 - val_loss: 0.0798 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 83ms/step - accuracy: 0.9963 - loss: 0.0213 - val_accuracy: 0.9782 - val_loss: 0.0768 - learning_rate: 2.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 80ms/step - accuracy: 0.9964 - loss: 0.0174 - val_accuracy: 0.9805 - val_loss: 0.0723 - learning_rate: 2.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 82ms/step - accuracy: 0.9980 - loss: 0.0129 - val_accuracy: 0.9832 - val_loss: 0.0749 - learning_rate: 2.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 83ms/step - accuracy: 0.9968 - loss: 0.0153 - val_accuracy: 0.9832 - val_loss: 0.0665 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9975 - loss: 0.0114 - val_accuracy: 0.9828 - val_loss: 0.0699 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9986 - loss: 0.0097 - val_accuracy: 0.9818 - val_loss: 0.0715 - learning_rate: 2.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 87ms/step - accuracy: 0.9981 - loss: 0.0092 - val_accuracy: 0.9807 - val_loss: 0.0751 - learning_rate: 2.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9983 - loss: 0.0089 - val_accuracy: 0.9818 - val_loss: 0.0761 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 87ms/step - accuracy: 0.9989 - loss: 0.0069 - val_accuracy: 0.9818 - val_loss: 0.0745 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization, LSTM, Embedding, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the Attention layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.dot(x, self.W) + self.b\n",
    "        e = tf.keras.backend.tanh(e)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "# Define the model architecture with attention, dropout, batch normalization, and regularization\n",
    "def create_lstm_model_with_regularization(vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_seq_length,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Increased LSTM units\n",
    "    model.add(Attention())  # Assuming your custom Attention layer\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(0.01), activation='relu'))  # Increased units in dense layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Assuming you already have X_train_fold, X_val_fold, y_train_fold, y_val_fold defined\n",
    "model = create_lstm_model_with_regularization(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_fold, y_train_fold,\n",
    "                    validation_data=(X_val_fold, y_val_fold),\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping, reduce_lr],\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "50c860d5-c5ad-4886-a87b-ffb733beeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.layers import Dropout, Dense, BatchNormalization, LSTM, Embedding, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming you have tokenizer, X_augmented, y_train, max_seq_length, and embedding_matrix_fasttext defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "da94fb9c-b840-4a24-a662-f9bb1573a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "\n",
    "def adversarial_training(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "        X_train_adv = projected_gradient_descent(model, X_train, eps=0.3, eps_iter=0.05)\n",
    "        history = model.fit(X_train_adv, y_train, validation_data=(X_val, y_val),\n",
    "                            epochs=1, batch_size=batch_size, verbose=1)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "6617f831-5479-4964-b68b-c6f86afb5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train, y_train, tokenizer, max_length, num_samples=1000):\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for text, label in zip(X_train, y_train):\n",
    "        text_sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "        augmented_texts.append(text_sequence)\n",
    "        augmented_labels.append(label)\n",
    "        for _ in range(num_samples):\n",
    "            permuted_text = np.random.permutation(text_sequence)\n",
    "            augmented_texts.append(permuted_text)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "    X_augmented = pad_sequences(augmented_texts, maxlen=max_length)\n",
    "    y_augmented = np.array(augmented_labels)\n",
    "\n",
    "    return X_augmented, y_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d9c8f990-3cc4-401b-ad63-8e29bdb162cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.dot(x, self.W) + self.b\n",
    "        e = tf.keras.backend.tanh(e)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return tf.keras.backend.sum(output, axis=1)\n",
    "\n",
    "def create_lstm_model_with_regularization(vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_seq_length,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Attention())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "aee5ba6f-1864-4410-baa8-15c066e2c3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmedma\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 84ms/step - accuracy: 0.8901 - loss: 0.7746 - val_accuracy: 0.9578 - val_loss: 0.1654\n",
      "Epoch 2/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 83ms/step - accuracy: 0.9594 - loss: 0.1430 - val_accuracy: 0.9660 - val_loss: 0.1128\n",
      "Epoch 3/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 81ms/step - accuracy: 0.9662 - loss: 0.1102 - val_accuracy: 0.9685 - val_loss: 0.1066\n",
      "Epoch 4/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 83ms/step - accuracy: 0.9780 - loss: 0.0815 - val_accuracy: 0.9639 - val_loss: 0.1178\n",
      "Epoch 5/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 86ms/step - accuracy: 0.9798 - loss: 0.0725 - val_accuracy: 0.9744 - val_loss: 0.0925\n",
      "Epoch 6/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9839 - loss: 0.0590 - val_accuracy: 0.9739 - val_loss: 0.0842\n",
      "Epoch 7/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 85ms/step - accuracy: 0.9871 - loss: 0.0490 - val_accuracy: 0.9685 - val_loss: 0.1220\n",
      "Epoch 8/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9871 - loss: 0.0455 - val_accuracy: 0.9768 - val_loss: 0.0895\n",
      "Epoch 9/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 83ms/step - accuracy: 0.9892 - loss: 0.0423 - val_accuracy: 0.9728 - val_loss: 0.0948\n",
      "Epoch 10/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 82ms/step - accuracy: 0.9911 - loss: 0.0356 - val_accuracy: 0.9778 - val_loss: 0.0834\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.9852 - loss: 0.0609\n",
      "Fold 1 validation accuracy: 0.9777576327323914, validation loss: 0.08339472115039825\n",
      "Epoch 1/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 86ms/step - accuracy: 0.8850 - loss: 0.8009 - val_accuracy: 0.9646 - val_loss: 0.1602\n",
      "Epoch 2/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9555 - loss: 0.1484 - val_accuracy: 0.9630 - val_loss: 0.1154\n",
      "Epoch 3/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9688 - loss: 0.1060 - val_accuracy: 0.9673 - val_loss: 0.1064\n",
      "Epoch 4/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9738 - loss: 0.0910 - val_accuracy: 0.9734 - val_loss: 0.1088\n",
      "Epoch 5/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 85ms/step - accuracy: 0.9823 - loss: 0.0663 - val_accuracy: 0.9823 - val_loss: 0.0689\n",
      "Epoch 6/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 82ms/step - accuracy: 0.9818 - loss: 0.0606 - val_accuracy: 0.9773 - val_loss: 0.0805\n",
      "Epoch 7/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 82ms/step - accuracy: 0.9865 - loss: 0.0507 - val_accuracy: 0.9791 - val_loss: 0.0777\n",
      "Epoch 8/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 83ms/step - accuracy: 0.9873 - loss: 0.0476 - val_accuracy: 0.9771 - val_loss: 0.0842\n",
      "Epoch 9/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 82ms/step - accuracy: 0.9895 - loss: 0.0403 - val_accuracy: 0.9669 - val_loss: 0.1012\n",
      "Epoch 10/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 82ms/step - accuracy: 0.9910 - loss: 0.0359 - val_accuracy: 0.9832 - val_loss: 0.0766\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.9928 - loss: 0.0373\n",
      "Fold 2 validation accuracy: 0.98320472240448, validation loss: 0.07655657827854156\n",
      "Epoch 1/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 85ms/step - accuracy: 0.8924 - loss: 0.7651 - val_accuracy: 0.9646 - val_loss: 0.1384\n",
      "Epoch 2/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.9587 - loss: 0.1463 - val_accuracy: 0.9664 - val_loss: 0.1059\n",
      "Epoch 3/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 84ms/step - accuracy: 0.9693 - loss: 0.1089 - val_accuracy: 0.9612 - val_loss: 0.1080\n",
      "Epoch 4/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 86ms/step - accuracy: 0.9755 - loss: 0.0851 - val_accuracy: 0.9682 - val_loss: 0.1182\n",
      "Epoch 5/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 86ms/step - accuracy: 0.9816 - loss: 0.0698 - val_accuracy: 0.9744 - val_loss: 0.0822\n",
      "Epoch 6/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 86ms/step - accuracy: 0.9849 - loss: 0.0551 - val_accuracy: 0.9716 - val_loss: 0.1075\n",
      "Epoch 7/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 88ms/step - accuracy: 0.9882 - loss: 0.0466 - val_accuracy: 0.9780 - val_loss: 0.0795\n",
      "Epoch 8/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 87ms/step - accuracy: 0.9884 - loss: 0.0452 - val_accuracy: 0.9755 - val_loss: 0.0816\n",
      "Epoch 9/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 87ms/step - accuracy: 0.9890 - loss: 0.0411 - val_accuracy: 0.9793 - val_loss: 0.0823\n",
      "Epoch 10/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 86ms/step - accuracy: 0.9882 - loss: 0.0401 - val_accuracy: 0.9680 - val_loss: 0.1330\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.9868 - loss: 0.0576\n",
      "Fold 3 validation accuracy: 0.967998206615448, validation loss: 0.13301542401313782\n",
      "Epoch 1/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 92ms/step - accuracy: 0.8793 - loss: 0.7762 - val_accuracy: 0.9287 - val_loss: 0.2251\n",
      "Epoch 2/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 88ms/step - accuracy: 0.9572 - loss: 0.1439 - val_accuracy: 0.9707 - val_loss: 0.1028\n",
      "Epoch 3/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9648 - loss: 0.1172 - val_accuracy: 0.9753 - val_loss: 0.0887\n",
      "Epoch 4/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 91ms/step - accuracy: 0.9754 - loss: 0.0837 - val_accuracy: 0.9759 - val_loss: 0.0830\n",
      "Epoch 5/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9810 - loss: 0.0703 - val_accuracy: 0.9705 - val_loss: 0.1008\n",
      "Epoch 6/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9847 - loss: 0.0561 - val_accuracy: 0.9768 - val_loss: 0.0734\n",
      "Epoch 7/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9852 - loss: 0.0540 - val_accuracy: 0.9657 - val_loss: 0.1464\n",
      "Epoch 8/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 88ms/step - accuracy: 0.9878 - loss: 0.0455 - val_accuracy: 0.9818 - val_loss: 0.0671\n",
      "Epoch 9/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 90ms/step - accuracy: 0.9901 - loss: 0.0406 - val_accuracy: 0.9809 - val_loss: 0.0784\n",
      "Epoch 10/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9892 - loss: 0.0427 - val_accuracy: 0.9750 - val_loss: 0.0830\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.9800 - loss: 0.0677\n",
      "Fold 4 validation accuracy: 0.9750340580940247, validation loss: 0.08299119025468826\n",
      "Epoch 1/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 90ms/step - accuracy: 0.8949 - loss: 0.7335 - val_accuracy: 0.9666 - val_loss: 0.1313\n",
      "Epoch 2/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 88ms/step - accuracy: 0.9571 - loss: 0.1511 - val_accuracy: 0.9723 - val_loss: 0.0934\n",
      "Epoch 3/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 87ms/step - accuracy: 0.9708 - loss: 0.1052 - val_accuracy: 0.9689 - val_loss: 0.0982\n",
      "Epoch 4/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 87ms/step - accuracy: 0.9769 - loss: 0.0789 - val_accuracy: 0.9741 - val_loss: 0.0845\n",
      "Epoch 5/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 91ms/step - accuracy: 0.9785 - loss: 0.0750 - val_accuracy: 0.9778 - val_loss: 0.0763\n",
      "Epoch 6/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9831 - loss: 0.0573 - val_accuracy: 0.9821 - val_loss: 0.0634\n",
      "Epoch 7/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 88ms/step - accuracy: 0.9864 - loss: 0.0494 - val_accuracy: 0.9714 - val_loss: 0.0988\n",
      "Epoch 8/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9895 - loss: 0.0437 - val_accuracy: 0.9746 - val_loss: 0.0918\n",
      "Epoch 9/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9893 - loss: 0.0409 - val_accuracy: 0.9517 - val_loss: 0.1489\n",
      "Epoch 10/10\n",
      "\u001b[1m551/551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 89ms/step - accuracy: 0.9915 - loss: 0.0362 - val_accuracy: 0.9782 - val_loss: 0.0839\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.9832 - loss: 0.0668\n",
      "Fold 5 validation accuracy: 0.978211522102356, validation loss: 0.08388260751962662\n",
      "Cross-validation completed.\n",
      "Average validation accuracy: 0.97644122838974\n",
      "Average validation loss: 0.0919681042432785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Convert y_train to numpy array\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, val_index in kf.split(X_augmented):\n",
    "    X_train_fold, X_val_fold = X_augmented[train_index], X_augmented[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_np[train_index], y_train_np[val_index]\n",
    "    \n",
    "    # Create model\n",
    "    model = create_lstm_model_with_regularization(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, validation_data=(X_val_fold, y_val_fold))\n",
    "    \n",
    "\n",
    "    # Evaluate model\n",
    "    loss, accuracy = model.evaluate(X_val_fold, y_val_fold)\n",
    "    print(f\"Fold {fold_no} validation accuracy: {accuracy}, validation loss: {loss}\")\n",
    "    val_accuracies.append(accuracy)\n",
    "    val_losses.append(loss)\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation completed.\")\n",
    "print(f\"Average validation accuracy: {np.mean(val_accuracies)}\")\n",
    "print(f\"Average validation loss: {np.mean(val_losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057c966-dfe1-434b-bad5-99e2d25d8011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [    0     1     2 ... 22027 22028 22029]\n",
      "Validation indices: [   34    44    54 ... 21971 21986 22001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmedma\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 81ms/step - accuracy: 0.8920 - loss: 0.7679 - val_accuracy: 0.9591 - val_loss: 0.1550\n",
      "Epoch 2/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 79ms/step - accuracy: 0.9601 - loss: 0.1427 - val_accuracy: 0.9532 - val_loss: 0.1750\n",
      "Epoch 3/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 79ms/step - accuracy: 0.9690 - loss: 0.1037 - val_accuracy: 0.9650 - val_loss: 0.1273\n",
      "Epoch 4/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9738 - loss: 0.0815 - val_accuracy: 0.9764 - val_loss: 0.0895\n",
      "Epoch 5/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 79ms/step - accuracy: 0.9813 - loss: 0.0664 - val_accuracy: 0.9728 - val_loss: 0.0930\n",
      "Epoch 6/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.9840 - loss: 0.0619 - val_accuracy: 0.9764 - val_loss: 0.0759\n",
      "Epoch 7/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 79ms/step - accuracy: 0.9869 - loss: 0.0502 - val_accuracy: 0.9691 - val_loss: 0.1082\n",
      "Epoch 8/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 79ms/step - accuracy: 0.9874 - loss: 0.0474 - val_accuracy: 0.9787 - val_loss: 0.0827\n",
      "Epoch 9/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 80ms/step - accuracy: 0.9883 - loss: 0.0446 - val_accuracy: 0.9814 - val_loss: 0.0694\n",
      "Epoch 10/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 80ms/step - accuracy: 0.9928 - loss: 0.0310 - val_accuracy: 0.9782 - val_loss: 0.0921\n",
      "Epoch 11/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.9914 - loss: 0.0350 - val_accuracy: 0.9800 - val_loss: 0.0912\n",
      "Epoch 12/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9931 - loss: 0.0277 - val_accuracy: 0.9773 - val_loss: 0.0917\n",
      "Epoch 13/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.9921 - loss: 0.0330 - val_accuracy: 0.9814 - val_loss: 0.0792\n",
      "Epoch 14/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9931 - loss: 0.0294 - val_accuracy: 0.9759 - val_loss: 0.0914\n",
      "Epoch 15/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9945 - loss: 0.0240 - val_accuracy: 0.9837 - val_loss: 0.0710\n",
      "Epoch 16/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.9957 - loss: 0.0223 - val_accuracy: 0.9719 - val_loss: 0.1086\n",
      "Epoch 17/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9947 - loss: 0.0236 - val_accuracy: 0.9787 - val_loss: 0.0920\n",
      "Epoch 18/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 81ms/step - accuracy: 0.9972 - loss: 0.0172 - val_accuracy: 0.9841 - val_loss: 0.0796\n",
      "Epoch 19/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9946 - loss: 0.0260 - val_accuracy: 0.9823 - val_loss: 0.0823\n",
      "Epoch 20/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 84ms/step - accuracy: 0.9928 - loss: 0.0264 - val_accuracy: 0.9837 - val_loss: 0.0743\n",
      "Epoch 21/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9944 - loss: 0.0223 - val_accuracy: 0.9846 - val_loss: 0.0689\n",
      "Epoch 22/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9971 - loss: 0.0130 - val_accuracy: 0.9823 - val_loss: 0.0721\n",
      "Epoch 23/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9955 - loss: 0.0215 - val_accuracy: 0.9778 - val_loss: 0.0875\n",
      "Epoch 24/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 81ms/step - accuracy: 0.9974 - loss: 0.0161 - val_accuracy: 0.9782 - val_loss: 0.1126\n",
      "Epoch 25/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9961 - loss: 0.0196 - val_accuracy: 0.9818 - val_loss: 0.0971\n",
      "Epoch 26/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9970 - loss: 0.0149 - val_accuracy: 0.9846 - val_loss: 0.0832\n",
      "Epoch 27/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9961 - loss: 0.0185 - val_accuracy: 0.9823 - val_loss: 0.1006\n",
      "Epoch 28/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9967 - loss: 0.0172 - val_accuracy: 0.9728 - val_loss: 0.1228\n",
      "Epoch 29/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9968 - loss: 0.0167 - val_accuracy: 0.9837 - val_loss: 0.0769\n",
      "Epoch 30/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 80ms/step - accuracy: 0.9971 - loss: 0.0160 - val_accuracy: 0.9846 - val_loss: 0.0741\n",
      "Epoch 31/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 83ms/step - accuracy: 0.9986 - loss: 0.0093 - val_accuracy: 0.9832 - val_loss: 0.0748\n",
      "Epoch 32/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9959 - loss: 0.0218 - val_accuracy: 0.9841 - val_loss: 0.0878\n",
      "Epoch 33/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9978 - loss: 0.0131 - val_accuracy: 0.9823 - val_loss: 0.0815\n",
      "Epoch 34/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9983 - loss: 0.0104 - val_accuracy: 0.9796 - val_loss: 0.1086\n",
      "Epoch 35/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 83ms/step - accuracy: 0.9977 - loss: 0.0136 - val_accuracy: 0.9791 - val_loss: 0.1037\n",
      "Epoch 36/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 83ms/step - accuracy: 0.9975 - loss: 0.0131 - val_accuracy: 0.9823 - val_loss: 0.0921\n",
      "Epoch 37/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 84ms/step - accuracy: 0.9965 - loss: 0.0168 - val_accuracy: 0.9868 - val_loss: 0.0684\n",
      "Epoch 38/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 83ms/step - accuracy: 0.9964 - loss: 0.0172 - val_accuracy: 0.9846 - val_loss: 0.0717\n",
      "Epoch 39/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9966 - loss: 0.0179 - val_accuracy: 0.9814 - val_loss: 0.0984\n",
      "Epoch 40/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 82ms/step - accuracy: 0.9959 - loss: 0.0193 - val_accuracy: 0.9814 - val_loss: 0.0928\n",
      "Epoch 41/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 84ms/step - accuracy: 0.9967 - loss: 0.0164 - val_accuracy: 0.9791 - val_loss: 0.0796\n",
      "Epoch 42/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 81ms/step - accuracy: 0.9971 - loss: 0.0132 - val_accuracy: 0.9809 - val_loss: 0.0715\n",
      "Epoch 43/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 85ms/step - accuracy: 0.9974 - loss: 0.0139 - val_accuracy: 0.9791 - val_loss: 0.0861\n",
      "Epoch 44/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 84ms/step - accuracy: 0.9965 - loss: 0.0168 - val_accuracy: 0.9768 - val_loss: 0.1137\n",
      "Epoch 45/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 84ms/step - accuracy: 0.9971 - loss: 0.0145 - val_accuracy: 0.9809 - val_loss: 0.0925\n",
      "Epoch 46/50\n",
      "\u001b[1m620/620\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 84ms/step - accuracy: 0.9979 - loss: 0.0105 - val_accuracy: 0.9791 - val_loss: 0.0945\n",
      "Epoch 47/50\n",
      "\u001b[1m308/620\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 80ms/step - accuracy: 0.9974 - loss: 0.0141"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Assuming you have defined X_augmented, y_train, tokenizer, max_seq_length, and embedding_matrix_fasttext\n",
    "\n",
    "# Define function to create individual models\n",
    "def create_lstm_model(vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_seq_length,\n",
    "                        trainable=False))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Attention())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define function to train and evaluate model on a fold\n",
    "def train_and_evaluate_model(X_train, y_train, X_val, y_val, vocab_size, embedding_dim, max_seq_length, embedding_matrix):\n",
    "    model = create_lstm_model(vocab_size, embedding_dim, max_seq_length, embedding_matrix)\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=1)\n",
    "    y_pred = model.predict_classes(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return model, accuracy\n",
    "\n",
    "# Initialize variables\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "models = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Train and evaluate models on each fold\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_augmented):\n",
    "    print(f\"Train indices: {train_index}\")\n",
    "    print(f\"Validation indices: {val_index}\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X_augmented[train_index], X_augmented[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Adjusted indexing\n",
    "    \n",
    "    # Continue with your training and evaluation\n",
    "\n",
    "    \n",
    "    # Train and evaluate individual model\n",
    "    model, accuracy = train_and_evaluate_model(X_train_fold, y_train_fold, X_val_fold, y_val_fold,\n",
    "                                               len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "    \n",
    "    models.append(('model_fold{}'.format(fold_no), model))  # Append trained model to list\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Fold {fold_no} validation accuracy: {accuracy}\")\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation completed.\")\n",
    "print(f\"Average validation accuracy: {np.mean(val_accuracies)}\")\n",
    "\n",
    "# Create VotingClassifier with individual models\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='hard')\n",
    "\n",
    "# Train VotingClassifier on full training data\n",
    "voting_classifier.fit(X_augmented, y_train)\n",
    "\n",
    "# Evaluate VotingClassifier (optional)\n",
    "y_pred = voting_classifier.predict(X_augmented)\n",
    "ensemble_accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Ensemble (VotingClassifier) accuracy on training data: {ensemble_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c45c2-4ea5-4bb0-8bb9-bcafd7a10299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = []\n",
    "val_accuracies = []\n",
    "predictions = []\n",
    "\n",
    "# Train and evaluate models on each fold\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_augmented):\n",
    "    X_train_fold, X_val_fold = X_augmented[train_index], X_augmented[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # Train and evaluate individual model\n",
    "    model, accuracy = train_and_evaluate_model(X_train_fold, y_train_fold, X_val_fold, y_val_fold,\n",
    "                                               len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "    \n",
    "    models.append(model)  # Append trained model to list\n",
    "    val_accuracies.append(accuracy)\n",
    "    \n",
    "    # Predict probabilities for blending\n",
    "    y_pred_proba = model.predict_proba(X_val_fold)\n",
    "    predictions.append(y_pred_proba)\n",
    "    \n",
    "    print(f\"Fold {fold_no} validation accuracy: {accuracy}\")\n",
    "    fold_no += 1\n",
    "\n",
    "print(\"Cross-validation completed.\")\n",
    "print(f\"Average validation accuracy: {np.mean(val_accuracies)}\")\n",
    "\n",
    "# Blend predictions (simple average)\n",
    "predictions = np.mean(predictions, axis=0)\n",
    "ensemble_predictions = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "# Evaluate ensemble predictions (optional)\n",
    "ensemble_accuracy = accuracy_score(y_val_fold, ensemble_predictions)\n",
    "print(f\"Ensemble (manual blending) accuracy on validation data: {ensemble_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e8243-8b27-46fe-80a8-2ecf1ef6bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "# Convert sequences back to text\n",
    "def sequences_to_texts(sequences, tokenizer):\n",
    "    return [' '.join([tokenizer.index_word.get(idx, '') for idx in seq if idx != 0]) for seq in sequences]\n",
    "\n",
    "# Convert texts back to sequences\n",
    "def texts_to_sequences(texts, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Assume tokenizer is already fitted on your data\n",
    "X_train_texts = sequences_to_texts(X_train, tokenizer)\n",
    "\n",
    "# Augment the training data\n",
    "X_augmented_texts = []\n",
    "for sentence in X_train_texts:\n",
    "    augmented_sentence = synonym_replacement(sentence, 2)\n",
    "    X_augmented_texts.append(augmented_sentence)\n",
    "\n",
    "# Convert the augmented data back to sequences\n",
    "X_augmented = texts_to_sequences(X_augmented_texts, tokenizer)\n",
    "\n",
    "# Pad the sequences\n",
    "X_augmented = pad_sequences(X_augmented, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Now you can proceed with the rest of your pipeline using X_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "d676004f-3a52-4c09-8d67-d3d57f7f8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "74bb2dd9-5b73-4fd5-8209-15ed45553793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym replacement for data augmentation\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "# Example usage for data augmentation\n",
    "X_train_texts = sequences_to_texts(X_train, tokenizer)\n",
    "X_augmented_texts = [synonym_replacement(sentence, 2) for sentence in X_train_texts]\n",
    "X_augmented = texts_to_sequences(X_augmented_texts, tokenizer)\n",
    "X_augmented = pad_sequences(X_augmented, maxlen=max_seq_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a451bfb-3b46-42aa-9176-792d739ecba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble prediction\n",
    "def ensemble_predict(models, X_test):\n",
    "    predictions = [model.predict(X_test) for model in models]\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "# Evaluate the ensemble on the test set\n",
    "y_pred = ensemble_predict(models, X_test)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b548b-0817-4da8-8fa5-dd9b840640f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent\n",
    "\n",
    "# Define adversarial training function\n",
    "def adversarial_training(model, X_train, y_train, epsilon=0.1):\n",
    "    # Create an instance of the Projected Gradient Descent (PGD) attack\n",
    "    pgd = projected_gradient_descent.ProjectedGradientDescent(model, sess=None)\n",
    "    \n",
    "    # Train the model with adversarial examples\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    for epoch in range(10):\n",
    "        # Generate adversarial examples\n",
    "        X_train_adv = pgd.generate(X_train, eps=epsilon)\n",
    "        \n",
    "        # Train on both original and adversarial examples\n",
    "        model.fit(X_train_adv, y_train, epochs=1, batch_size=32, verbose=1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Assuming you have a model instance `model`\n",
    "# Call the adversarial_training function\n",
    "model = adversarial_training(model, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372dd0f-87c7-4305-9feb-8fc3cab31e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Function to create model (required for KerasClassifier)\n",
    "def create_model():\n",
    "    return create_lstm_model_with_regularization(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "\n",
    "# Create the KerasClassifier\n",
    "model1 = KerasClassifier(build_fn=create_model, epochs=40, batch_size=32, verbose=0)\n",
    "model2 = KerasClassifier(build_fn=create_model, epochs=40, batch_size=32, verbose=0)\n",
    "\n",
    "# Combine models into an ensemble\n",
    "ensemble_model = VotingClassifier(estimators=[('model1', model1), ('model2', model2)], voting='soft')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy = ensemble_model.score(X_val, y_val)\n",
    "print(f\"Ensemble model validation accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dccea32-7539-4bb0-83d6-ae2189fba4b0",
   "metadata": {},
   "source": [
    "5. Monitoring and Logging\n",
    "Implementing monitoring and logging helps track the model's performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fa814dfb-457f-4067-a1f1-b2d7e7887e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='model.log', level=logging.INFO)\n",
    "\n",
    "# Log model performance\n",
    "logging.info(f\"Training Accuracy: {history.history['accuracy'][-1]}, Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3225f4-fac1-4d34-a465-bdde70e34186",
   "metadata": {},
   "source": [
    "6. Ensemble Methods\n",
    "Ensemble methods combine multiple models to improve prediction reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "733d99b8-39ee-48b1-8dd1-79eecfba54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.8956 - loss: 0.2624 - val_accuracy: 0.9713 - val_loss: 0.0907\n",
      "Epoch 2/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9743 - loss: 0.0772 - val_accuracy: 0.9737 - val_loss: 0.0777\n",
      "Epoch 3/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9856 - loss: 0.0431 - val_accuracy: 0.9766 - val_loss: 0.0639\n",
      "Epoch 4/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 54ms/step - accuracy: 0.9932 - loss: 0.0226 - val_accuracy: 0.9811 - val_loss: 0.0544\n",
      "Epoch 5/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9961 - loss: 0.0127 - val_accuracy: 0.9826 - val_loss: 0.0570\n",
      "Epoch 6/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9978 - loss: 0.0081 - val_accuracy: 0.9777 - val_loss: 0.0665\n",
      "Epoch 7/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9968 - loss: 0.0091 - val_accuracy: 0.9717 - val_loss: 0.0933\n",
      "Epoch 8/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9973 - loss: 0.0085 - val_accuracy: 0.9827 - val_loss: 0.0563\n",
      "Epoch 9/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9975 - loss: 0.0085 - val_accuracy: 0.9853 - val_loss: 0.0568\n",
      "Epoch 10/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 0.9840 - val_loss: 0.0782\n",
      "Epoch 11/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 54ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 0.9811 - val_loss: 0.0706\n",
      "Epoch 12/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 56ms/step - accuracy: 0.9979 - loss: 0.0072 - val_accuracy: 0.9708 - val_loss: 0.1276\n",
      "Epoch 13/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 54ms/step - accuracy: 0.9966 - loss: 0.0115 - val_accuracy: 0.9829 - val_loss: 0.0682\n",
      "Epoch 14/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 0.9840 - val_loss: 0.0811\n",
      "Epoch 15/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9986 - loss: 0.0041 - val_accuracy: 0.9800 - val_loss: 0.0732\n",
      "Epoch 16/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9995 - loss: 0.0026 - val_accuracy: 0.9739 - val_loss: 0.0896\n",
      "Epoch 17/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 54ms/step - accuracy: 0.9986 - loss: 0.0045 - val_accuracy: 0.9844 - val_loss: 0.0676\n",
      "Epoch 18/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 54ms/step - accuracy: 0.9999 - loss: 6.9901e-04 - val_accuracy: 0.9798 - val_loss: 0.0707\n",
      "Epoch 19/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9966 - loss: 0.0092 - val_accuracy: 0.9788 - val_loss: 0.0931\n",
      "Epoch 20/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 54ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 0.9818 - val_loss: 0.0828\n",
      "Epoch 21/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 0.9768 - val_loss: 0.0989\n",
      "Epoch 22/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 0.9808 - val_loss: 0.0717\n",
      "Epoch 23/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 5.0956e-04 - val_accuracy: 0.9837 - val_loss: 0.0818\n",
      "Epoch 24/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 1.4973e-04 - val_accuracy: 0.9720 - val_loss: 0.1844\n",
      "Epoch 25/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9657 - val_loss: 0.2059\n",
      "Epoch 26/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9981 - loss: 0.0064 - val_accuracy: 0.9813 - val_loss: 0.0956\n",
      "Epoch 27/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 0.9826 - val_loss: 0.0875\n",
      "Epoch 28/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9838 - val_loss: 0.0792\n",
      "Epoch 29/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 2.4743e-04 - val_accuracy: 0.9846 - val_loss: 0.0836\n",
      "Epoch 30/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 6.9658e-05 - val_accuracy: 0.9842 - val_loss: 0.0880\n",
      "Epoch 31/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 3.8944e-05 - val_accuracy: 0.9846 - val_loss: 0.0909\n",
      "Epoch 32/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 2.5697e-05 - val_accuracy: 0.9840 - val_loss: 0.0944\n",
      "Epoch 33/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 1.8592e-05 - val_accuracy: 0.9842 - val_loss: 0.0975\n",
      "Epoch 34/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 1.2503e-05 - val_accuracy: 0.9844 - val_loss: 0.1006\n",
      "Epoch 35/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 9.2195e-06 - val_accuracy: 0.9846 - val_loss: 0.1044\n",
      "Epoch 36/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 6.5176e-06 - val_accuracy: 0.9846 - val_loss: 0.1072\n",
      "Epoch 37/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 4.6748e-06 - val_accuracy: 0.9847 - val_loss: 0.1112\n",
      "Epoch 38/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 3.6179e-06 - val_accuracy: 0.9849 - val_loss: 0.1137\n",
      "Epoch 39/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 2.2744e-06 - val_accuracy: 0.9851 - val_loss: 0.1185\n",
      "Epoch 40/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 1.5694e-06 - val_accuracy: 0.9851 - val_loss: 0.1220\n",
      "Epoch 1/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 53ms/step - accuracy: 0.8270 - loss: 0.4097 - val_accuracy: 0.9315 - val_loss: 0.1815\n",
      "Epoch 2/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 51ms/step - accuracy: 0.9449 - loss: 0.1603 - val_accuracy: 0.9544 - val_loss: 0.1296\n",
      "Epoch 3/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 51ms/step - accuracy: 0.9597 - loss: 0.1179 - val_accuracy: 0.9628 - val_loss: 0.1118\n",
      "Epoch 4/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 51ms/step - accuracy: 0.9673 - loss: 0.0940 - val_accuracy: 0.9702 - val_loss: 0.0897\n",
      "Epoch 5/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 51ms/step - accuracy: 0.9764 - loss: 0.0697 - val_accuracy: 0.9717 - val_loss: 0.0820\n",
      "Epoch 6/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9783 - loss: 0.0653 - val_accuracy: 0.9722 - val_loss: 0.0819\n",
      "Epoch 7/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9802 - loss: 0.0571 - val_accuracy: 0.9755 - val_loss: 0.0715\n",
      "Epoch 8/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9886 - loss: 0.0364 - val_accuracy: 0.9771 - val_loss: 0.0667\n",
      "Epoch 9/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9907 - loss: 0.0287 - val_accuracy: 0.9778 - val_loss: 0.0622\n",
      "Epoch 10/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9938 - loss: 0.0229 - val_accuracy: 0.9777 - val_loss: 0.0695\n",
      "Epoch 11/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.9940 - loss: 0.0187 - val_accuracy: 0.9789 - val_loss: 0.0682\n",
      "Epoch 12/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9954 - loss: 0.0147 - val_accuracy: 0.9778 - val_loss: 0.0705\n",
      "Epoch 13/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9979 - loss: 0.0091 - val_accuracy: 0.9689 - val_loss: 0.0978\n",
      "Epoch 14/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9972 - loss: 0.0096 - val_accuracy: 0.9806 - val_loss: 0.0715\n",
      "Epoch 15/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9977 - loss: 0.0079 - val_accuracy: 0.9795 - val_loss: 0.0688\n",
      "Epoch 16/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9982 - loss: 0.0068 - val_accuracy: 0.9758 - val_loss: 0.0865\n",
      "Epoch 17/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.9962 - loss: 0.0131 - val_accuracy: 0.9788 - val_loss: 0.0954\n",
      "Epoch 18/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 0.9800 - val_loss: 0.0825\n",
      "Epoch 19/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 0.9800 - val_loss: 0.0969\n",
      "Epoch 20/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 49ms/step - accuracy: 0.9987 - loss: 0.0038 - val_accuracy: 0.9773 - val_loss: 0.1173\n",
      "Epoch 21/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.9795 - val_loss: 0.1159\n",
      "Epoch 22/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.4447e-04 - val_accuracy: 0.9811 - val_loss: 0.1258\n",
      "Epoch 23/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 5.4912e-05 - val_accuracy: 0.9809 - val_loss: 0.1378\n",
      "Epoch 24/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 2.7176e-05 - val_accuracy: 0.9802 - val_loss: 0.1512\n",
      "Epoch 25/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 1.9869e-05 - val_accuracy: 0.9797 - val_loss: 0.1603\n",
      "Epoch 26/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 1.2519e-05 - val_accuracy: 0.9798 - val_loss: 0.1675\n",
      "Epoch 27/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 9.8131e-06 - val_accuracy: 0.9798 - val_loss: 0.1755\n",
      "Epoch 28/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 4.6590e-06 - val_accuracy: 0.9793 - val_loss: 0.1832\n",
      "Epoch 29/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 3.6602e-06 - val_accuracy: 0.9791 - val_loss: 0.1903\n",
      "Epoch 30/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 0.9793 - val_loss: 0.0811\n",
      "Epoch 31/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9955 - loss: 0.0123 - val_accuracy: 0.9795 - val_loss: 0.0920\n",
      "Epoch 32/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 0.9786 - val_loss: 0.0993\n",
      "Epoch 33/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9748 - val_loss: 0.1242\n",
      "Epoch 34/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - accuracy: 0.9966 - loss: 0.0112 - val_accuracy: 0.9768 - val_loss: 0.0984\n",
      "Epoch 35/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 0.9997 - loss: 0.0023 - val_accuracy: 0.9813 - val_loss: 0.1001\n",
      "Epoch 36/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 1.6287e-04 - val_accuracy: 0.9829 - val_loss: 0.1033\n",
      "Epoch 37/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 4.7837e-05 - val_accuracy: 0.9829 - val_loss: 0.1070\n",
      "Epoch 38/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 3.0208e-05 - val_accuracy: 0.9833 - val_loss: 0.1126\n",
      "Epoch 39/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 1.7770e-05 - val_accuracy: 0.9835 - val_loss: 0.1181\n",
      "Epoch 40/40\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 1.2955e-05 - val_accuracy: 0.9835 - val_loss: 0.1234\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The estimator Sequential should be a classifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[229], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Combine models into an ensemble\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Combine models into an ensemble\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m VotingClassifier(estimators\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_fasttext\u001b[39m\u001b[38;5;124m'\u001b[39m, model1), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_glove\u001b[39m\u001b[38;5;124m'\u001b[39m, model2)], voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate the ensemble\u001b[39;00m\n\u001b[0;32m     21\u001b[0m y_pred_ensemble \u001b[38;5;241m=\u001b[39m ensemble\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:346\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    344\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, transformed_y, sample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:73\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get common fit operations.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     names, clfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_estimators()\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:297\u001b[0m, in \u001b[0;36m_BaseHeterogeneousEnsemble._validate_estimators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m estimators:\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_estimator_type(est):\n\u001b[1;32m--> 297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    298\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe estimator \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m should be a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    299\u001b[0m                 est\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_estimator_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m    300\u001b[0m             )\n\u001b[0;32m    301\u001b[0m         )\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m names, estimators\n",
      "\u001b[1;31mValueError\u001b[0m: The estimator Sequential should be a classifier."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create and compile the models\n",
    "model1 = create_lstm_model(len(tokenizer.word_index) + 1, 300, max_seq_length, embedding_matrix_fasttext)\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2 = create_lstm_model(len(tokenizer.word_index) + 1, 100, max_seq_length, embedding_matrix_glove)\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the models\n",
    "model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=40, batch_size=32)\n",
    "model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=40, batch_size=32)\n",
    "\n",
    "# Combine models into an ensemble\n",
    "\n",
    "# Combine models into an ensemble\n",
    "ensemble = VotingClassifier(estimators=[('lstm_fasttext', model1), ('lstm_glove', model2)], voting='soft')\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "print(f\"Ensemble Test Accuracy: {accuracy_score(y_test, y_pred_ensemble):.2f}\")\n",
    "print(classification_report(y_test, y_pred_ensemble))\n",
    "# Train the ensemble model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614e920-2660-4fac-bc8f-daef59d9bf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
